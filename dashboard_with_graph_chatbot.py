import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import time
import os
from dotenv import load_dotenv
from openai import OpenAI

# Import custom modules
import llm_system_massage_manager
import draw_gauge
import draw_spider_graph
import draw_bar_chart
import connect_to_google_sheet
import consts
import anigmas
from class_school_info import SchoolInfo
from graph_manager import Gauge_Graph_type, Spider_Graph_type, Bar_Chart_Graph_type
from system_prompt import return_prompt
# ×˜×¢×™× ×ª ××©×ª× ×™ ×”×¡×‘×™×‘×” ××§×•×‘×¥ .env
load_dotenv()

# ×§×‘×œ×ª ×”-API key ××”××©×ª× ×™× ×•×”×’×“×¨×ª ×”×œ×§×•×—
api_key = os.getenv("OPENAI_API_KEY")
if api_key:
    openai_client = OpenAI(api_key=api_key)
else:
    openai_client = OpenAI()  # ×™×ª×›×Ÿ ×©×™×¢×‘×•×“ ×× ×™×© API key ×‘×¡×‘×™×‘×”

# Page Configuration
st.set_page_config(
    page_title="×“××©×‘×•×¨×“ × ×™×ª×•×— × ×ª×•× ×™×",
    page_icon="ğŸ“Š",
    layout="wide",
)

# Add RTL support and custom styling
st.markdown(
    """
    <style>
    /* RTL Support */
    h1, h2, h3, h4, h5, h6, p, div {
        text-align: right;
        direction: rtl;
    }
    
    /* Dashboard Layout Styling */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    
    /* Card styling for graphs */
    .stPlotlyChart {
        background-color: white;
        border-radius: 10px;
        padding: 10px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin-bottom: 1rem;
    }
    
    /* Chat container styling */
    .chat-container {
        background-color: white;
        border-radius: 10px;
        padding: 15px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        height: 100%;
    }
    
    /* Sidebar styling */
    .css-1d391kg {
        text-align: right;
        direction: rtl;
    }
    
    /* Selectbox RTL support */
    div[data-testid="stSelectbox"] {
        text-align: right;
        direction: rtl;
    }
    
    div[data-testid="stSelectbox"] > label > p {
        text-align: right;
        direction: rtl;
    }
    
    /* Make sure markdown in chat messages appears correctly in RTL */
    .element-container .stMarkdown {
        direction: rtl;
        text-align: right;
    }
    
    /* Styling for markdown elements in chat */
    .element-container .stMarkdown h1,
    .element-container .stMarkdown h2,
    .element-container .stMarkdown h3,
    .element-container .stMarkdown h4,
    .element-container .stMarkdown h5,
    .element-container .stMarkdown h6,
    .element-container .stMarkdown p,
    .element-container .stMarkdown ul,
    .element-container .stMarkdown ol {
        direction: rtl;
        text-align: right;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Initialize data
def init():
    data = connect_to_google_sheet.return_data()
    df = pd.DataFrame(data)
    consts.init_st_session_global_average_and_research_average(df)
    consts.init_heg_avg(df)
    return df

# Initialize session state for chat and graph data
if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "graph_data" not in st.session_state:
    st.session_state.graph_data = {
        "risc": None,
        "ici": None, 
        "spider": None,
        "selected_school": None
    }

# ×”×¤×•× ×§×¦×™×” ×”××©×•×¤×¨×ª ×œ×§×‘×œ×ª ×ª×©×•×‘×” ×-OpenAI ×¢× ×ª××™×›×” ×‘× ×ª×•× ×™ ×”×’×¨×¤×™× ×•×‘×¡×˜×¨×™××™× ×’
def get_openai_response(prompt, system_prompt, history, graph_data, stream=False):
    # ×™×¦×™×¨×ª ×ª×§×¦×™×¨ ×©×œ × ×ª×•× ×™ ×”×’×¨×¤×™× ×›×“×™ ×œ×”×¢×‘×™×¨ ××•×ª× ×œ×¦'××˜×‘×•×˜
    graph_summary = ""
    if graph_data["selected_school"]:
        graph_summary += f"×”× ×ª×•× ×™× ×©×œ×”×œ×Ÿ ××ª×™×™×—×¡×™× ×œ×‘×™×ª ×”×¡×¤×¨: {graph_data['selected_school']}\n\n"
    
    if graph_data["risc"] is not None:
        graph_summary += f"× ×ª×•× ×™ ×’×¨×£ ×—×•×¡×Ÿ (RISC):\n"
        graph_summary += f"×¢×¨×š × ×•×›×—×™: {graph_data['risc']['value']:.2f}\n"
        graph_summary += f"×××•×¦×¢ ××¨×¦×™: {graph_data['risc']['global_avg']:.2f}\n"
        graph_summary += f"×××•×¦×¢ ××—×§×¨×™: {graph_data['risc']['research_avg']:.2f}\n\n"
        
    if graph_data["ici"] is not None:
        graph_summary += f"× ×ª×•× ×™ ×’×¨×£ ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™ (ICI):\n"
        graph_summary += f"×¢×¨×š × ×•×›×—×™: {graph_data['ici']['value']:.2f}\n"
        graph_summary += f"×××•×¦×¢ ××¨×¦×™: {graph_data['ici']['global_avg']:.2f}\n"
        graph_summary += f"×××•×¦×¢ ××—×§×¨×™: {graph_data['ici']['research_avg']:.2f}\n\n"
        
    if graph_data["spider"] is not None:
        graph_summary += f"× ×ª×•× ×™ ×’×¨×£ ×¨×“××¨ (Spider) ×œ×ª×¤×™×¡×•×ª ×–××Ÿ:\n"
        
        for category, values in graph_data["spider"].items():
            formatted_category = category.replace("future_", "").replace("_past", "").replace("_present", "")
            graph_summary += f"×§×˜×’×•×¨×™×”: {formatted_category}\n"
            graph_summary += f"  ×¢×¨×š × ×•×›×—×™: {values['current']:.2f}\n"
            graph_summary += f"  ×××•×¦×¢ ××¨×¦×™: {values['global']:.2f}\n"
            graph_summary += f"  ×××•×¦×¢ ××—×§×¨×™: {values['research']:.2f}\n"
    
    # ×”×•×¡×¤×ª ×”× ×—×™×•×ª ×¡×¤×¦×™×¤×™×•×ª ×œ×’×¨×¤×™× ×‘×¤×¨×•××¤×˜ ×”××¢×¨×›×ª
    graph_system_prompt = system_prompt + ""

# ×ª×•×¡×¤×ª ×—×©×•×‘×”: ××¢×ª×” ×¢×œ×™×š ×œ×”×ª×™×™×—×¡ ×’× ×œ× ×ª×•× ×™ ×”×’×¨×¤×™× ×”××•×¦×’×™× ×œ××©×ª××© ×•×œ××¤×©×¨ × ×™×ª×•×— ××¢××™×§ ×©×œ ×”× ×ª×•× ×™× ×”×—×–×•×ª×™×™×. ×›××©×¨ ×”××©×ª××© ×©×•××œ ×©××œ×•×ª ×œ×’×‘×™ ×”×’×¨×¤×™×, ×¢×œ×™×š ×œ× ×ª×— ××ª ×”× ×ª×•× ×™× ×•×œ×¡×¤×§ ×ª×•×‘× ×•×ª ×¨×œ×•×•× ×˜×™×•×ª.

# ×œ×”×œ×Ÿ ×¡×•×’×™ ×”×’×¨×¤×™× ×”××•×¦×’×™× ×œ××©×ª××©:
# 1. ×’×¨×£ ×—×•×¡×Ÿ (RISC) - ××¨××” ××ª ×¨××ª ×”×—×•×¡×Ÿ ×”× ×¤×©×™ ×©×œ ×‘×™×ª ×”×¡×¤×¨ ×‘×”×©×•×•××” ×œ×××•×¦×¢ ××¨×¦×™ ×•×××•×¦×¢ ××—×§×¨×™.
# 2. ×’×¨×£ ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™ (ICI) - ××¨××” ××ª ×¨××ª ××™×§×•×“ ×”×©×œ×™×˜×” ×”×¤× ×™××™ ×©×œ ×‘×™×ª ×”×¡×¤×¨ ×‘×”×©×•×•××” ×œ×××•×¦×¢ ××¨×¦×™ ×•××—×§×¨×™.
# 3. ×’×¨×£ ×¨×“××¨ (Spider) - ××¨××” ××ª ×ª×¤×™×¡×•×ª ×”×–××Ÿ ×”×©×•× ×•×ª ×©×œ ×‘×™×ª ×”×¡×¤×¨ (×¢×‘×¨ ×—×™×•×‘×™, ×¢×‘×¨ ×©×œ×™×œ×™, ×”×•×•×” ×”×“×•× ×™×¡×˜×™, ×”×•×•×” ×¤××˜××œ×™, ×¢×ª×™×“).

# ×›××©×¨ ××ª×” ×× ×ª×— ××ª ×”×’×¨×¤×™×, ×©×™× ×œ×‘:
# - ×”×©×•×•××” ×‘×™×Ÿ ×”×¢×¨×š ×”× ×•×›×—×™ ×œ×××•×¦×¢×™× - ×”×× ×‘×™×ª ×”×¡×¤×¨ ×’×‘×•×”/× ××•×š ××”×××•×¦×¢?
# - ××’××•×ª ×‘×•×œ×˜×•×ª - ×”×× ×™×© ××“×“×™× ×©×‘×”× ×‘×™×ª ×”×¡×¤×¨ ×‘×•×œ×˜ ×‘××™×•×—×“ ×œ×˜×•×‘×” ××• ×œ×¨×¢×”?
# - ×§×©×¨×™× ×‘×™×Ÿ ×”××“×“×™× ×”×©×•× ×™× - ×”×× ×™×© ×§×©×¨ ×‘×™×Ÿ ×—×•×¡×Ÿ × ××•×š ×œ×ª×¤×™×¡×ª ×–××Ÿ ××¡×•×™××ª?
# - ×”××œ×¦×•×ª ××¢×©×™×•×ª ×œ×”×ª×¢×¨×‘×•×ª - ××” ××¤×©×¨ ×œ×¢×©×•×ª ×›×“×™ ×œ×©×¤×¨ ××ª ×”××“×“×™× ×”× ××•×›×™×?

# ×›××• ×›×Ÿ, ×× ×”××©×ª××© ××‘×§×© ×“×•×— ×œ×× ×”×œ ×‘×™×ª ×¡×¤×¨, ×¦×•×¨ ×˜×§×¡×˜ ××•×ª×× ×œ×× ×”×œ/×ª ×‘×™×ª ×¡×¤×¨ ×¢×œ ×‘×¡×™×¡ ×”×ª×‘× ×™×ª ×”×‘××”, ×ª×•×š ×”×—×œ×¤×ª [×©× ×‘×™×ª ×”×¡×¤×¨] ×‘×©× ×‘×™×ª ×”×¡×¤×¨ ×”××¡×•×™×, ×•×”×ª×××ª ×”×ª×•×›×Ÿ ×œ×¤×™ ×¡×•×’ ×•×¨××ª ×”×ª×•×¦××•×ª ×©×”×ª×§×‘×œ×• ×‘×©×œ×•×©×ª ×”××“×“×™× ×”×‘××™×: ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™, ×—×•×¡×Ÿ, ×•×ª×¤×™×¡×ª ×–××Ÿ.

# ×”×˜×§×¡×˜ ×¦×¨×™×š ×œ×›×œ×•×œ:
# 1. ×¤×ª×™×—×” ×©××¦×™×’×” ××ª ×ª×›× ×™×ª "×”×¦×™×¨ ×”×× ×˜×œ×™" ×•×”×©××œ×•×Ÿ ×©××™×œ××• ×”×ª×œ××™×“×™×
# 2. ××–×›×•×¨ ×©×œ ×”××“×“×™× ×”×ª×™××•×¨×˜×™×™× (××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™, ×—×•×¡×Ÿ, ×•×ª×¤×™×¡×ª ×–××Ÿ) ×¢× ××–×›×•×¨ ××œ× ×©×œ ×”××××¨×™× ×”×¡×¤×¦×™×¤×™×™× ×”×‘××™×:
#    - ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™ ×¢"×¤ ×”×ª×™××•×¨×™×” ×©×œ × ×•×‘×™×¦×§×™ ×•×¡×˜×¨×™×§×œ× ×“ (Nowicki, S., & Strickland, B. R. (1973). A locus of control scale for children. *Journal of Consulting and Clinical Psychology, 40*(1), 148-154)
#    - ×—×•×¡×Ÿ ×•×”×ª××•×“×“×•×ª ×¢× ××ª×’×¨×™× ×¢"×¤ ×§×•× ×•×¨ ×•×“×•×™×“×¡×•×Ÿ (Connor, K. M., & Davidson, J. R. T. (2003). Development of a new resilience scale: The Connor-Davidson Resilience Scale (CD-RISC). *Depression and Anxiety, 18*(2), 76-82)
#    - ×ª×¤×™×¡×ª ×–××Ÿ ×©×œ ×–×™××‘×¨×“×• ×•×‘×•×™×“ ×¢"×¤ (Zimbardo, P. G., & Boyd, J. N. (1999). Putting time in perspective: A valid, reliable individual-differences metric. *Journal of Personality and Social Psychology, 77*(6), 1271-1288)
# 3. ×”×ª×™×™×—×¡×•×ª ×œ×ª×•×¦××•×ª ×”×¡×¤×¦×™×¤×™×•×ª ×©×œ ×‘×™×ª ×”×¡×¤×¨ ×™×—×¡×™×ª ×œ×××•×¦×¢ ×”××¨×¦×™, ×ª×•×š ×©×™××•×© ×‘×¡×§××œ×” ×”×‘××”:
#    * 0-9% ××ª×—×ª/××¢×œ ×œ×××•×¦×¢ = "××¢×˜ ××ª×—×ª/××¢×œ ×œ×××•×¦×¢"
#    * 10-19% ××ª×—×ª/××¢×œ ×œ×××•×¦×¢ = "××ª×—×ª/××¢×œ ×œ×××•×¦×¢ ×‘××™×“×” ××¡×•×™××ª"
#    * 20%+ ××ª×—×ª/××¢×œ ×œ×××•×¦×¢ = "××ª×—×ª/××¢×œ ×œ×××•×¦×¢ ×‘××•×¤×Ÿ ××©××¢×•×ª×™"
# 4. ×”×ª×™×™×—×¡×•×ª ×—×™×•×‘×™×ª ×•××¢×¦×™××” ×œ××“×“ ×”×’×‘×•×” ×‘×™×•×ª×¨ (××• ×œ×¤×—×•×ª ×‘×××•×¦×¢)
# 5. ×©××œ×•×ª ×××•×§×“×•×ª ×œ×—×©×™×‘×” ×‘×”×ª×× ×œ××“×“ ×©×‘×• ×‘×™×ª ×”×¡×¤×¨ × ××¦× ×‘×ª×•×¦××” ×”×—×œ×©×” ×‘×™×•×ª×¨
# 6. ×¡×™×•× ×”××–××™×Ÿ ×œ×”×ª×‘×•× × ×•×ª ××©×•×ª×¤×ª ×•×¤×¢×•×œ×”

# ×—×©×•×‘ ×××•×“: ×¤×¨××˜ ××ª ×›×œ ×ª×©×•×‘×•×ª×™×š ×‘-Markdown. ×”×©×ª××© ×‘×›×•×ª×¨×•×ª (#, ##), ×¨×©×™××•×ª (*, -), ×”×“×’×©×•×ª (**×˜×§×¡×˜ ××•×“×’×©**), ×˜×‘×œ××•×ª ×•××œ×× ×˜×™× ××—×¨×™× ×©×œ Markdown ×›×“×™ ×œ×”×¤×•×š ××ª ×”×ª×©×•×‘×” ×©×œ×š ×œ×‘×¨×•×¨×”, ×××•×¨×’× ×ª ×•× ×•×—×” ×œ×§×¨×™××”.
# """
    
    # ×”×•×¡×¤×ª ×ª×§×¦×™×¨ ×”×’×¨×¤×™× ×œ×¤×¨×•××¤×˜ ×”××©×ª××©
    user_prompt = f"""history: {history}

current prompt: {prompt}

× ×ª×•× ×™ ×”×’×¨×¤×™× ×”××•×¦×’×™× ×‘×“××©×‘×•×¨×“:
{graph_summary}
"""
    
    try:
        if stream:
            # ×”×—×–×¨ ××•×‘×™×™×§×˜ ×¡×˜×¨×™××™× ×’ ×©× ×™×ª×Ÿ ×œ××™×˜×¨×¦×™×”
            response_stream = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": graph_system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                stream=True
            )
            return response_stream
        else:
            # ×”×—×–×¨×ª ×ª×©×•×‘×” ××œ××” (×œ×œ× ×¡×˜×¨×™××™× ×’)
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": graph_system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )
            return response.choices[0].message.content
    except Exception as e:
        return f"××™×¨×¢×” ×©×’×™××” ×‘×¢×ª ×”×ª×§×©×•×¨×ª ×¢× OpenAI: {str(e)}"

# Streamed response generator for chatbot
def response_generator(prompt, df, graph_data,school_info):
    data = df.to_markdown()
    system_prompt = return_prompt(school_info) #llm_system_massage_manager.get_first_system_prompt(data)
    
    # ×§×‘×œ×ª ×”×ª×©×•×‘×” ×‘×××¦×¢×•×ª ×”-API Key ××§×•×‘×¥ .env ×•×”×¢×‘×¨×ª × ×ª×•× ×™ ×”×’×¨×¤×™×
    # ×¢× ×¡×˜×¨×™××™× ×’ ××•×¤×¢×œ
    response_stream = get_openai_response(prompt, system_prompt, st.session_state.messages, graph_data, stream=True)
    
    full_response = ""
    # ×œ×•×œ××” ×¢×œ ××™×¨×•×¢×™ ×”×¡×˜×¨×™× ×-OpenAI
    for chunk in response_stream:
        if chunk.choices and hasattr(chunk.choices[0], "delta") and hasattr(chunk.choices[0].delta, "content"):
            content = chunk.choices[0].delta.content
            if content:
                full_response += content
                yield content  # ×©×œ×™×—×ª ×›×œ ×—×œ×§ ×©××ª×§×‘×œ ××™×“×™×ª ×œ×ª×¦×•×’×”
    
    # ×”×—×–×¨×ª ×”×ª×©×•×‘×” ×”××œ××” ×œ×¦×•×¨×š ×©××™×¨×” ×‘×”×™×¡×˜×•×¨×™×”
    return full_response

# Main Dashboard
def main():
    st.title("×“××©×‘×•×¨×“ × ×™×ª×•×— × ×ª×•× ×™× ğŸ“Š")
    st.markdown("### × ×™×ª×•×— × ×ª×•× ×™× ×—×™× ×•×›×™×™× ×¢× ×’×¨×¤×™× ×•××™× ×˜×¨××§×¦×™×”")
    
    # Load data
    try:
        df = init()
    except Exception as e:
        st.error(f"××™×¨×¢×” ×©×’×™××” ×‘×˜×¢×™× ×ª ×”× ×ª×•× ×™×: {e}")
        df = pd.DataFrame()  # Empty dataframe as fallback
    
    # Sidebar for filtering
    with st.sidebar:
        st.header("×¡×™× ×•×Ÿ × ×ª×•× ×™×")
        
        # School selection
        selected_school = None
        if not df.empty and 'school' in df.columns:
            unique_schools = df["school"].unique().tolist()
            selected_school = st.selectbox("×‘×—×¨ ×‘×™×ª ×¡×¤×¨:", unique_schools)
            filtered_df = df[df['school'] == selected_school]
            st.session_state.graph_data["selected_school"] = selected_school
        else:
            # Demo data if no real data is available
            filtered_df = df
            st.warning("×œ× × ××¦××• × ×ª×•× ×™× ×œ×¡×™× ×•×Ÿ")
    
    # Create layout - 2 rows
    # First row: 3 graphs
    # Second row: chatbot
    row1_col1, row1_col2, row1_col3 = st.columns(3)
    
    # Generate sample graphs if no data
    if filtered_df.empty:
        # Sample data for demonstration
        with row1_col1:
            st.markdown("### ××“×“ ×—×•×¡×Ÿ")
            fig1 = generate_sample_gauge()
            st.plotly_chart(fig1, use_container_width=True)
            
            # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×œ×“×•×’××” ×œ×¦'××˜×‘×•×˜
            st.session_state.graph_data["risc"] = {
                "value": 3.7,
                "global_avg": 3.5,
                "research_avg": 3.8
            }
            
        with row1_col2:
            st.markdown("### ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™")
            fig2 = generate_sample_gauge(value=3.4, title="××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™")
            st.plotly_chart(fig2, use_container_width=True)
            
            # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×œ×“×•×’××” ×œ×¦'××˜×‘×•×˜
            st.session_state.graph_data["ici"] = {
                "value": 3.4,
                "global_avg": 3.2,
                "research_avg": 3.6
            }
            
        with row1_col3:
            st.markdown("### × ×ª×•× ×™ ×¦×™×¨ ×”×–××Ÿ")
            fig3 = generate_sample_spider()
            st.plotly_chart(fig3, use_container_width=True)
            
            # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×œ×“×•×’××” ×œ×¦'××˜×‘×•×˜
            st.session_state.graph_data["spider"] = {
                "negetive_past": {"current": 3.2, "global": 3.0, "research": 2.8},
                "positive_past": {"current": 4.1, "global": 3.8, "research": 4.0},
                "fatalic_present": {"current": 2.8, "global": 3.0, "research": 2.5},
                "hedonistic_present": {"current": 3.5, "global": 3.2, "research": 3.0},
                "future": {"current": 3.9, "global": 3.7, "research": 4.0},
            }
    else:
        # Real graphs from data
        try:
            school_info = SchoolInfo(filtered_df)
            
            with row1_col1:
                st.markdown("### ××“×“ ×—×•×¡×Ÿ")
                fig_risc = school_info.get_fig_risc("×—×•×¡×Ÿ")
                st.plotly_chart(fig_risc, use_container_width=True)
                
                # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×××™×ª×™×™× ×œ×¦'××˜×‘×•×˜
                st.session_state.graph_data["risc"] = {
                    "value": school_info.risc,
                    "global_avg": st.session_state.global_average["risc"],
                    "research_avg": st.session_state.research_average["risc"]
                }
                
            with row1_col2:
                st.markdown("### ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™")
                fig_ici = school_info.get_fig_ici("××™×§×•×“ ×©×œ×™×˜×”")
                st.plotly_chart(fig_ici, use_container_width=True)
                
                # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×××™×ª×™×™× ×œ×¦'××˜×‘×•×˜
                st.session_state.graph_data["ici"] = {
                    "value": school_info.ici,
                    "global_avg": st.session_state.global_average["ici"],
                    "research_avg": st.session_state.research_average["ici"]
                }
                
            with row1_col3:
                st.markdown("### ×”×ª×¤×œ×’×•×ª ×œ×¤×™ ×××“×™ ×–××Ÿ")
                fig_spider = school_info.get_fig_spider()
                st.plotly_chart(fig_spider, use_container_width=True)
                
                # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×××™×ª×™×™× ×œ×¦'××˜×‘×•×˜
                anigmas_dict = school_info.return_anigmas_result_as_dict()
                st.session_state.graph_data["spider"] = {
                    "future_negetive_past": {
                        "current": anigmas_dict["future_negetive_past"],
                        "global": st.session_state.global_average["future_negetive_past"],
                        "research": st.session_state.research_average["future_negetive_past"]
                    },
                    "future_positive_past": {
                        "current": anigmas_dict["future_positive_past"],
                        "global": st.session_state.global_average["future_positive_past"],
                        "research": st.session_state.research_average["future_positive_past"]
                    },
                    "future_fatalic_present": {
                        "current": anigmas_dict["future_fatalic_present"],
                        "global": st.session_state.global_average["future_fatalic_present"],
                        "research": st.session_state.research_average["future_fatalic_present"]
                    },
                    "future_hedonistic_present": {
                        "current": anigmas_dict["future_hedonistic_present"],
                        "global": st.session_state.global_average["future_hedonistic_present"],
                        "research": st.session_state.research_average["future_hedonistic_present"]
                    },
                    "future_future": {
                        "current": anigmas_dict["future_future"],
                        "global": st.session_state.global_average["future_future"],
                        "research": st.session_state.research_average["future_future"]
                    }
                }
        except Exception as e:
            st.error(f"×©×’×™××” ×‘×¢×ª ×™×¦×™×¨×ª ×”×’×¨×¤×™×: {e}")
    
    # Divider
    st.markdown("---")
    
    # Chatbot section with API key information
    st.markdown("### ×¦'××˜×‘×•×˜ ×œ× ×™×ª×•×— × ×ª×•× ×™× ğŸ¤–")
    
    # Show API status
    if api_key:
        st.success("×—×™×‘×•×¨ ×œ-OpenAI API ×¤×¢×™×œ")
    else:
        st.warning("×œ× × ××¦× API key ×©×œ OpenAI. ×”×¦'××˜×‘×•×˜ ×¢×©×•×™ ×œ× ×œ×¤×¢×•×œ ×›×¨××•×™.")
    
    st.markdown("×©××œ ×©××œ×•×ª ×¢×œ ×”× ×ª×•× ×™× ×”××•×¦×’×™× ×‘×’×¨×¤×™× ××• ×¢×œ ×”×ª×•×¦××•×ª ×”×›×œ×œ×™×•×ª")
    
    # ×”×¦×¢×•×ª ×œ×©××œ×•×ª ×œ××©×ª××©
    suggested_questions = [
        "××” ×”××¦×‘ ×©×œ ×”×ª×œ××™×“×™× ××‘×—×™× ×ª ×—×•×¡×Ÿ?",
        "××™×š ××™×§×•×“ ×”×©×œ×™×˜×” ×”×¤× ×™××™ ×©×œ ×‘×™×ª ×”×¡×¤×¨ ××©×ª×•×•×” ×œ×××•×¦×¢ ×”××¨×¦×™?",
        "××™×–×” ××“×“ ×ª×¤×™×¡×ª ×–××Ÿ ×”×›×™ ×‘×•×œ×˜ ×œ×—×™×•×‘?",
        "××” ×”××©××¢×•×ª ×©×œ ×”× ×ª×•× ×™× ×‘×’×¨×£ ×”×¨×“××¨?",
        "×”×›×Ÿ ×“×•×— ×œ×× ×”×œ ×‘×™×ª ×”×¡×¤×¨"
    ]
    
    # ×××©×§ ××©×ª××© ×©××¨××” ×”×¦×¢×•×ª ×œ×©××œ×•×ª
    st.markdown("#### ×©××œ×•×ª ×œ×“×•×’××”:")
    cols = st.columns(3)
    for i, question in enumerate(suggested_questions):
        col_idx = i % 3
        if cols[col_idx].button(question, key=f"question_{i}"):
            # Add user message to chat history
            st.session_state.messages.append({"role": "user", "content": question})
            
            # ××¦×™×’×™× ××ª ×©××œ×ª ×”××©×ª××©
            with st.chat_message("user"):
                st.markdown(question)
                
            # ××¦×™×’×™× ××ª ×ª×©×•×‘×ª ×”×¦'××˜×‘×•×˜ ×¢× ×¡×˜×¨×™××™× ×’ ×‘×–××Ÿ ×××ª
            with st.chat_message("assistant"):
                data = filtered_df.to_markdown() if not filtered_df.empty else "××™×Ÿ × ×ª×•× ×™× ×–××™× ×™×"
                system_prompt = system_prompt #llm_system_massage_manager.get_first_system_prompt(data)
                
                # ×©×™××•×© ×‘×¡×˜×¨×™××™× ×’ ×¢×‘×•×¨ ×©××œ×•×ª ×“×•×’××”
                response_stream = get_openai_response(
                    question, 
                    system_prompt, 
                    st.session_state.messages[:-1], 
                    st.session_state.graph_data,
                    stream=True
                )
                
                # ××§×•× ×œ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª
                message_placeholder = st.empty()
                full_response = ""
                
                # ×œ×•×œ××” ×¢×œ ××™×¨×•×¢×™ ×”×¡×˜×¨×™× ×-OpenAI ×•×”×¦×’×ª× ×‘×–××Ÿ ×××ª
                for chunk in response_stream:
                    if chunk.choices and hasattr(chunk.choices[0], "delta") and hasattr(chunk.choices[0].delta, "content"):
                        content = chunk.choices[0].delta.content
                        if content:
                            full_response += content
                            # ××¦×™×’ ××ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª ×¢× ×¡××Ÿ ××”×‘×”×‘
                            message_placeholder.markdown(full_response + "â–Œ")
                
                # ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××œ××” ×”×¡×•×¤×™×ª (×œ×œ× ×”×¡××Ÿ)
                message_placeholder.markdown(full_response)
            
            # ×”×•×¡×¤×ª ×”×ª×©×•×‘×” ×œ×”×™×¡×˜×•×¨×™×”
            st.session_state.messages.append({"role": "assistant", "content": full_response})
    
    # ×”×¦×’×ª ×”×™×¡×˜×•×¨×™×™×ª ×¦'××˜ (×¨×§ ××”×™×¡×˜×•×¨×™×” ×§×•×“××ª)
    messages_to_display = st.session_state.messages[:-2] if len(st.session_state.messages) >= 2 else []
    if messages_to_display:
        st.markdown("#### ×”×™×¡×˜×•×¨×™×™×ª ×¦'××˜:")
        chat_container = st.container()
        with chat_container:
            for message in messages_to_display:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
    
    # Input for chat
    if prompt := st.chat_input("×©××œ ×©××œ×” ×¢×œ ×”× ×ª×•× ×™×..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # ××¦×™×’×™× ××ª ×©××œ×ª ×”××©×ª××©
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ××¦×™×’×™× ××ª ×ª×©×•×‘×ª ×”×¦'××˜×‘×•×˜ ×¢× ×¡×˜×¨×™××™× ×’ ×‘×–××Ÿ ×××ª
        with st.chat_message("assistant"):
            # ××§×•× ×œ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª
            message_placeholder = st.empty()
            full_response = ""
            
            # ×§×‘×œ×ª ×ª×©×•×‘×” ××”×¦'××˜×‘×•×˜ ×¢× ×¡×˜×¨×™××™× ×’
            response_stream = get_openai_response(
                prompt, 
                llm_system_massage_manager.get_first_system_prompt(filtered_df.to_markdown() if not filtered_df.empty else "××™×Ÿ × ×ª×•× ×™× ×–××™× ×™×"), 
                st.session_state.messages[:-1], 
                st.session_state.graph_data,
                stream=True
            )
            
            # ×œ×•×œ××” ×¢×œ ××™×¨×•×¢×™ ×”×¡×˜×¨×™× ×•×”×¦×’×ª ×”×ª×©×•×‘×” ×‘×–××Ÿ ×××ª
            for chunk in response_stream:
                if chunk.choices and hasattr(chunk.choices[0], "delta") and hasattr(chunk.choices[0].delta, "content"):
                    content = chunk.choices[0].delta.content
                    if content:
                        full_response += content
                        # ××¦×™×’ ××ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª ×¢× ×¡××Ÿ ××”×‘×”×‘
                        message_placeholder.markdown(full_response + "â–Œ")
            
            # ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××œ××” ×”×¡×•×¤×™×ª (×œ×œ× ×”×¡××Ÿ)
            message_placeholder.markdown(full_response)
        
        # ×”×•×¡×¤×ª ×”×ª×©×•×‘×” ×œ×”×™×¡×˜×•×¨×™×”
        st.session_state.messages.append({"role": "assistant", "content": full_response})

# Helper functions for sample graphs if no data is available
def generate_sample_gauge(value=3.7, title="×—×•×¡×Ÿ"):
    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=value,
        title={'text': title, 'font': {'size': 24}},
        gauge={
            'axis': {'range': [1, 5]},
            'bar': {'color': "#437742"},
            'steps': [
                {'range': [1, 2], 'color': "#f4f4f4"},
                {'range': [2, 3], 'color': "#f0f0f0"},
                {'range': [3, 4], 'color': "#e8e8e8"},
                {'range': [4, 5], 'color': "#e0e0e0"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 3.5
            }
        }
    ))
    fig.update_layout(height=300)
    return fig

def generate_sample_bar_chart():
    categories = ["×§×˜×’×•×¨×™×” ×", "×§×˜×’×•×¨×™×” ×‘", "×§×˜×’×•×¨×™×” ×’", "×§×˜×’×•×¨×™×” ×“"]
    values = [4.2, 3.8, 2.5, 3.9]
    
    df = pd.DataFrame({"name": categories, "value": values})
    fig = px.bar(df, x='name', y='value', title="×”×ª×¤×œ×’×•×ª ×¦×™×•× ×™×")
    
    # Add reference lines
    fig.add_hline(y=3.5, line=dict(color="#F37321"), annotation_text="×××•×¦×¢ ××—×§×¨×™")
    fig.add_hline(y=3.2, line=dict(color="#1F3B91"), annotation_text="×××•×¦×¢ ××¨×¦×™")
    
    fig.update_layout(height=300)
    return fig

def generate_sample_spider():
    categories = ["×ª×¤×™×¡×ª ×¢×‘×¨ ××¢×›×‘×ª", "×¢×‘×¨ ×›×ª×©×ª×™×ª ×—×™×•×‘×™×ª", "×“×˜×¨××™× ×¡×˜×™×•×ª",
                 "×¡×™×¤×•×§ ××™×™×“×™", "×¢×ª×™×“"]
    
    fig = go.Figure()
    
    # Current values
    fig.add_trace(go.Scatterpolar(
        r=[3.2, 4.1, 2.8, 3.5, 3.9],
        theta=categories,
        fill='none',
        name='×××•×¦×¢ × ×•×›×—×™',
        line=dict(color='#437742')
    ))
    
    # Global average
    fig.add_trace(go.Scatterpolar(
        r=[3.0, 3.8, 3.0, 3.2, 3.7],
        theta=categories,
        fill='none',
        name='×××•×¦×¢ ××¨×¦×™',
        line=dict(color='#1F3B91')
    ))
    
    # Research average
    fig.add_trace(go.Scatterpolar(
        r=[2.8, 4.0, 2.5, 3.0, 4.0],
        theta=categories,
        fill='none',
        name='×××•×¦×¢ ××—×§×¨×™',
        line=dict(color='#F37321')
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[1, 5]
            )
        ),
        showlegend=True,
        height=300
    )
    
    return fig

# Run the main dashboard
if __name__ == "__main__":
    main()