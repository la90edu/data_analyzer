import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import time
import os
from dotenv import load_dotenv
from openai import OpenAI

# Import custom modules
import llm_system_massage_manager
import llm_gpt
import draw_gauge
import draw_spider_graph
import draw_bar_chart
import connect_to_google_sheet
import consts
import anigmas
from class_school_info import SchoolInfo
from graph_manager import Gauge_Graph_type, Spider_Graph_type, Bar_Chart_Graph_type
import prompts
# ×˜×¢×™× ×ª ××©×ª× ×™ ×”×¡×‘×™×‘×” ××§×•×‘×¥ .env
load_dotenv()

# ×§×‘×œ×ª ×”-API key ××”××©×ª× ×™× ×•×”×’×“×¨×ª ×”×œ×§×•×—
api_key = os.getenv("OPENAI_API_KEY")
if api_key:
    openai_client = OpenAI(api_key=api_key)
else:
    openai_client = OpenAI()  # ×™×ª×›×Ÿ ×©×™×¢×‘×•×“ ×× ×™×© API key ×‘×¡×‘×™×‘×”

# Page Configuration
st.set_page_config(
    page_title="×“××©×‘×•×¨×“ × ×™×ª×•×— × ×ª×•× ×™×",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items=None
)

# Force light mode regardless of device settings
st.markdown("""
<script>
    // Force light mode
    document.querySelector('html').classList.remove('dark');
    document.querySelector('html').classList.add('light');
    
    // Override any system preferences for dark mode
    const style = document.createElement('style');
    style.textContent = `
        html, body, .stApp {
            color-scheme: light !important;
            background-color: white !important;
            color: #333333 !important;
        }
        
        /* Ensure dark mode doesn't activate via media query */
        @media (prefers-color-scheme: dark) {
            html, body, .stApp {
                color-scheme: light !important;
                background-color: white !important;
                color: #333333 !important;
            }
        }
    `;
    document.head.append(style);
</script>
""", unsafe_allow_html=True)

# Add RTL support and custom styling
st.markdown(
    """
    <style>
    /* Force Light Theme */
    .stApp {
        background-color: #ffffff !important;
    }
    
    /* Override any dark mode styling */
    [data-testid="stSidebar"], .main {
        background-color: #ffffff !important;
        color: #333333 !important;
    }
    
    /* RTL Support */
    h1, h2, h3, h4, h5, h6, p, div {
        text-align: right;
        direction: rtl;
    }
    
    /* Sidebar open by default */
    [data-testid="stSidebar"][aria-expanded="true"] {
        width: 300px !important;
        background-color: #ffffff !important;
    }
    
    [data-testid="stSidebar"][aria-expanded="false"] {
        width: 300px !important;
        margin-left: -300px;
        background-color: #ffffff !important;
    }
    
    /* Force sidebar to be open on page load */
    section[data-testid="stSidebar"] {
        display: block !important;
        opacity: 1 !important;
        transform: none !important;
        background-color: #ffffff !important;
    }
    
    /* Dashboard Layout Styling */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    
    /* Card styling for graphs */
    .stPlotlyChart {
        background-color: white;
        border-radius: 10px;
        padding: 10px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin-bottom: 1rem;
    }
    
    /* Chat container styling */
    .chat-container {
        background-color: white;
        border-radius: 10px;
        padding: 15px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        height: 100%;
    }
    
    /* ×©×™×¤×•×¨ ×’×•×“×œ ×”×¢××•×“×•×ª ×œ×× ×™×¢×ª ×“×—×™×¡×” ×©×œ ×”×’×¨×¤×™× */
    .row-widget.stButton, [data-testid="stVerticalBlock"] > div {
        padding: 0 !important;
        margin: 0 !important;
    }
    
    /* ×•×™×“×•× ×©×”×’×¨×¤×™× ××§×‘×œ×™× ××ª ×”×¨×•×—×‘ ×”××ª××™× */
    [data-testid="column"] {
        width: auto !important;
        min-width: 0;
        flex: 1;
    }
    
    /* ×¢×™×¦×•×‘ ×¨×›×™×‘ ×‘×—×™×¨×ª ×‘×™×ª ×¡×¤×¨ ×‘×—×œ×§ ×”×¢×œ×™×•×Ÿ */
    .school-selector {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 10px;
        margin-bottom: 20px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
    }
    
    .school-selector .stSelectbox {
        max-width: 400px;
        margin: 0 auto;
    }
    
    /* Override any color in explanation boxes to ensure light mode */
    .explanation-box {
        background-color: transparent !important;
        border: none !important;
        padding: 15px !important;
        color: #333333 !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Initialize data
def init():
    data = connect_to_google_sheet.return_data()
    df = pd.DataFrame(data)
    consts.init_st_session_global_average_and_research_average(df)
    consts.init_heg_avg(df)
    return df

#knowledge_files=

# Initialize session state for chat and graph data
if "messages" not in st.session_state:
    st.session_state.messages = []
    # Adicionar uma mensagem inicial de boas-vindas do assistente
    welcome_message = "×‘×¨×•×›×™× ×”×‘××™× ×œ×¦'××˜×‘×•×˜ ×œ× ×™×ª×•×— × ×ª×•× ×™×! ğŸ‘‹\n\n×× ×™ ×›××Ÿ ×›×“×™ ×œ×¢×–×•×¨ ×œ×š ×œ×”×‘×™×Ÿ ××ª ×”× ×ª×•× ×™× ×©×œ×š ×•×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª. ××ª×” ×™×›×•×œ ×œ×©××•×œ ××•×ª×™ ×¢×œ:\n\n- × ×™×ª×•×— ×©×œ ××“×“×™ ×”×—×•×¡×Ÿ\n- ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™\n- ×ª×¤×™×¡×•×ª ×–××Ÿ ×©×œ ×”×ª×œ××™×“×™×\n- ×”××œ×¦×•×ª ×œ×”××©×š\n\n×× ××ª×” ×¨×•×¦×” ×œ×¨××•×ª ××ª ×”× ×ª×•× ×™× ×‘×¦×•×¨×” ×•×™×–×•××œ×™×ª, ×œ×—×¥ ×¢×œ ×›×¤×ª×•×¨ '×”×¨××” ×œ×™ ×’×¨×¤×™×' ×‘×—×œ×§ ×”×¢×œ×™×•×Ÿ ×©×œ ×”×¢××•×“."
    st.session_state.messages.append({"role": "assistant", "content": welcome_message})
    
if "graph_data" not in st.session_state:
    st.session_state.graph_data = {
        "risc": None,
        "ici": None, 
        "spider": None,
        "selected_school": None
    }

# Initialize session state for chart visibility if not already set
if "show_charts" not in st.session_state:
    st.session_state.show_charts = {
        "risc": False,
        "ici": False,
        "spider": False,
        "sample_risc": False,
        "sample_ici": False,
        "sample_spider": False,
        "summary": False  # ××¦×‘ ×¢×‘×•×¨ ×”×¡×‘×¨ ××¡×›×
    }

# Flag to check if summary has been added to chatbot
if "summary_added_to_chat" not in st.session_state:
    st.session_state.summary_added_to_chat = False

# Function to toggle chart visibility
def toggle_chart_visibility(chart_name):
    st.session_state.show_charts[chart_name] = not st.session_state.show_charts[chart_name]

# Function to show all charts
def show_all_charts():
    for chart in st.session_state.show_charts:
        if chart != "summary":  # ×œ× ×œ×”×¤×¢×™×œ ××ª ×”×”×¡×‘×¨ ×”××¡×›× ×‘××•×¤×Ÿ ××•×˜×•××˜×™
            st.session_state.show_charts[chart] = True

# Initialize session state for graph explanations
if "graph_explanations" not in st.session_state:
    st.session_state.graph_explanations = {
        "risc": {"show": False, "explanation": ""},
        "ici": {"show": False, "explanation": ""},
        "spider": {"show": False, "explanation": ""},
        "summary": {"show": False, "explanation": ""}  # ×”×•×¡×¤×ª ××§×•× ×œ××—×¡×•×Ÿ ×”×”×¡×‘×¨ ×”××¡×›×
    }

# ×”×¤×•× ×§×¦×™×” ×”××©×•×¤×¨×ª ×œ×§×‘×œ×ª ×ª×©×•×‘×” ×-OpenAI ×¢× ×ª××™×›×” ×‘× ×ª×•× ×™ ×”×’×¨×¤×™× ×•×‘×¡×˜×¨×™××™× ×’
def get_openai_response(prompt, system_prompt, history, graph_data, stream=False):
    # ×™×¦×™×¨×ª ×ª×§×¦×™×¨ ×©×œ × ×ª×•× ×™ ×”×’×¨×¤×™× ×›×“×™ ×œ×”×¢×‘×™×¨ ××•×ª× ×œ×¦'××˜×‘×•×˜
    graph_summary = ""
    if graph_data["selected_school"]:
        graph_summary += f"×”× ×ª×•× ×™× ×©×œ×”×œ×Ÿ ××ª×™×™×—×¡×™× ×œ×‘×™×ª ×”×¡×¤×¨: {graph_data['selected_school']}\n\n"
    
    if graph_data["risc"] is not None:
        graph_summary += f"× ×ª×•× ×™ ×’×¨×£ ×—×•×¡×Ÿ :\n"
        graph_summary += f"×¢×¨×š × ×•×›×—×™: {graph_data['risc']['value']:.2f}\n"
        graph_summary += f"×××•×¦×¢ ××¨×¦×™: {graph_data['risc']['global_avg']:.2f}\n"
        #graph_summary += f"×××•×¦×¢ ××—×§×¨×™: {graph_data['risc']['research_avg']:.2f}\n\n"
        
    if graph_data["ici"] is not None:
        graph_summary += f"× ×ª×•× ×™ ×’×¨×£ ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™ (ICI):\n"
        graph_summary += f"×¢×¨×š × ×•×›×—×™: {graph_data['ici']['value']:.2f}\n"
        graph_summary += f"×××•×¦×¢ ××¨×¦×™: {graph_data['ici']['global_avg']:.2f}\n"
       # graph_summary += f"×××•×¦×¢ ××—×§×¨×™: {graph_data['ici']['research_avg']:.2f}\n\n"
        
    if graph_data["spider"] is not None:
        graph_summary += f"×œ×ª×¤×™×¡×•×ª ×–××Ÿ:\n"
        
        for category, values in graph_data["spider"].items():
            formatted_category = category.replace("future_", "").replace("_past", "").replace("_present", "")
            graph_summary += f"×§×˜×’×•×¨×™×”: {formatted_category}\n"
            graph_summary += f"  ×¢×¨×š × ×•×›×—×™: {values['current']:.2f}\n"
            graph_summary += f"  ×××•×¦×¢ ××¨×¦×™: {values['global']:.2f}\n"
            #graph_summary += f"  ×××•×¦×¢ ××—×§×¨×™: {values['research']:.2f}\n"
    
    # ×”×•×¡×¤×ª ×”× ×—×™×•×ª ×¡×¤×¦×™×¤×™×•×ª ×œ×’×¨×¤×™× ×‘×¤×¨×•××¤×˜ ×”××¢×¨×›×ª
    graph_system_prompt = system_prompt + ""

    
    # ×”×•×¡×¤×ª ×ª×§×¦×™×¨ ×”×’×¨×¤×™× ×œ×¤×¨×•××¤×˜ ×”××©×ª××©
    user_prompt = f"""history: {history}

current prompt: {prompt}

× ×ª×•× ×™ ×”×’×¨×¤×™× ×”××•×¦×’×™× ×‘×“××©×‘×•×¨×“:
{graph_summary}
"""
    
    try:
        if stream:
            # ×”×—×–×¨ ××•×‘×™×™×§×˜ ×¡×˜×¨×™××™× ×’ ×©× ×™×ª×Ÿ ×œ××™×˜×¨×¦×™×”
            response_stream = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": graph_system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.05,  # ×”×•×¡×¤×ª ×˜××¤×¨×˜×•×¨×” × ××•×›×” ×œ×ª×©×•×‘×•×ª ×™×•×ª×¨ ×××•×§×“×•×ª
                stream=True
            )
            return response_stream
        else:
            # ×”×—×–×¨×ª ×ª×©×•×‘×” ××œ××” (×œ×œ× ×¡×˜×¨×™××™× ×’)
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": graph_system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.05  # ×”×•×¡×¤×ª ×˜××¤×¨×˜×•×¨×” × ××•×›×” ×œ×ª×©×•×‘×•×ª ×™×•×ª×¨ ×××•×§×“×•×ª
            )
            return response.choices[0].message.content
    except Exception as e:
        return f"××™×¨×¢×” ×©×’×™××” ×‘×¢×ª ×”×ª×§×©×•×¨×ª ×¢× OpenAI: {str(e)}"

# Streamed response generator for chatbot
def response_generator(prompt, df, graph_data,school_info):
    data = df.to_markdown()
    system_prompt = prompts.return_good_prompt() #llm_system_massage_manager.get_first_system_prompt(data)
    
    # ×§×‘×œ×ª ×”×ª×©×•×‘×” ×‘×××¦×¢×•×ª ×”-API Key ××§×•×‘×¥ .env ×•×”×¢×‘×¨×ª × ×ª×•× ×™ ×”×’×¨×¤×™×
    # ×¢× ×¡×˜×¨×™××™× ×’ ××•×¤×¢×œ
    response_stream = get_openai_response(prompt, system_prompt, st.session_state.messages, graph_data, stream=True)
    
    full_response = ""
    # ×œ×•×œ××” ×¢×œ ××™×¨×•×¢×™ ×”×¡×˜×¨×™× ×-OpenAI
    for chunk in response_stream:
        if chunk.choices and hasattr(chunk.choices[0], "delta") and hasattr(chunk.choices[0].delta, "content"):
            content = chunk.choices[0].delta.content
            if content:
                full_response += content
                yield content  # ×©×œ×™×—×ª ×›×œ ×—×œ×§ ×©××ª×§×‘×œ ××™×“×™×ª ×œ×ª×¦×•×’×”
    
    # ×”×—×–×¨×ª ×”×ª×©×•×‘×” ×”××œ××” ×œ×¦×•×¨×š ×©××™×¨×” ×‘×”×™×¡×˜×•×¨×™×”
    return full_response

# ×¤×•× ×§×¦×™×” ×œ×§×‘×œ×ª ×”×¡×‘×¨ ×¢×œ ×’×¨×£ ×¡×¤×¦×™×¤×™
def get_graph_explanation(graph_type, graph_data, school_info=None):
    """
    ××§×‘×œ×ª ×”×¡×‘×¨ ×¢×œ ×’×¨×£ ×¡×¤×¦×™×¤×™ ×××•×“×œ ×”×©×¤×” ×‘×¡×˜×¨×™××™× ×’
    
    Args:
        graph_type: ×¡×•×’ ×”×’×¨×£ (risc, ici, spider)
        graph_data: ×”× ×ª×•× ×™× ×©×œ ×”×’×¨×£
        school_info: ××•×‘×™×™×§×˜ SchoolInfo ×× ×§×™×™×
    
    Returns:
        ×”×¡×‘×¨ ×˜×§×¡×˜×•××œ×™ ××¤×•×¨×˜ ×¢×œ ×”×’×¨×£ ×•××©××¢×•×ª×•
    """
    if graph_type == "risc":
        prompt = f"""
        ×”×¡×‘×¨ ×‘×‘×§×©×” ××ª × ×ª×•× ×™ ×’×¨×£ ×”×—×•×¡×Ÿ ×”×‘××™×:
        ×¢×¨×š × ×•×›×—×™: {graph_data['value']:.2f}
        ×××•×¦×¢ ××¨×¦×™: {graph_data['global_avg']:.2f}
        
        ××” ×”××©××¢×•×ª ×©×œ ×”× ×ª×•× ×™× ×”××œ×”? ×”×× ×”×¢×¨×š ×’×‘×•×” ××• × ××•×š ×‘×™×—×¡ ×œ×××•×¦×¢? ××” ×–×” ××•××¨ ×¢×œ ×‘×™×ª ×”×¡×¤×¨?
        ×”×ª×™×™×—×¡ ×œ×”×©×œ×›×•×ª ×”×—×™× ×•×›×™×•×ª ×•×”×—×‘×¨×ª×™×•×ª ×©×œ ×”×ª×•×¦××•×ª. ×ª×Ÿ ×”××œ×¦×•×ª ×œ×”××©×š ×‘×”×ª×‘×¡×¡ ×¢×œ ×”× ×ª×•× ×™×.
        """
    elif graph_type == "ici":
        prompt = f"""
        ×”×¡×‘×¨ ×‘×‘×§×©×” ××ª × ×ª×•× ×™ ×’×¨×£ ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™ (ICI) ×”×‘××™×:
        ×¢×¨×š × ×•×›×—×™: {graph_data['value']:.2f}
        ×××•×¦×¢ ××¨×¦×™: {graph_data['global_avg']:.2f}
        
        ××” ×”××©××¢×•×ª ×©×œ ×”× ×ª×•× ×™× ×”××œ×”? ×”×× ×”×¢×¨×š ×’×‘×•×” ××• × ××•×š ×‘×™×—×¡ ×œ×××•×¦×¢? ××” ×–×” ××•××¨ ×¢×œ ×‘×™×ª ×”×¡×¤×¨?
        ×”×ª×™×™×—×¡ ×œ×”×©×œ×›×•×ª ×”×—×™× ×•×›×™×•×ª ×•×”×—×‘×¨×ª×™×•×ª ×©×œ ×”×ª×•×¦××•×ª. ×ª×Ÿ ×”××œ×¦×•×ª ×œ×”××©×š ×‘×”×ª×‘×¡×¡ ×¢×œ ×”× ×ª×•× ×™×.
        """
    elif graph_type == "spider":
        prompt = """
        ×”×¡×‘×¨ ×‘×‘×§×©×” ××ª × ×ª×•× ×™ ×’×¨×£ ×ª×¤×™×¡×•×ª ×”×–××Ÿ (×’×¨×£ ×”×¢×›×‘×™×©) ×”××•×¦×’.
        ××”×™ ×”××©××¢×•×ª ×©×œ ×›×œ ××—×“ ××”×××“×™×? ××™×œ×• ×××“×™× ×‘×•×œ×˜×™× ×œ×˜×•×‘×” ×•××™×œ×• ×œ×¨×¢×”?
        ××”×Ÿ ×”×”×©×œ×›×•×ª ×”×—×™× ×•×›×™×•×ª ×•×”×¤×¡×™×›×•×œ×•×’×™×•×ª ×©×œ ×”×ª×•×¦××•×ª?
        ×ª×Ÿ ×”××œ×¦×•×ª ×œ×”××©×š ×‘×”×ª×‘×¡×¡ ×¢×œ ×”× ×ª×•× ×™×.
        """
        
        # ×”×•×¡×¤×ª ×¤×™×¨×•×˜ ×¢×œ ×”×§×˜×’×•×¨×™×•×ª ×‘×’×¨×£ ×”×¢×›×‘×™×© ×× ×™×© × ×ª×•× ×™×
        if graph_data:
            prompt += "\n\n× ×ª×•× ×™ ×”×§×˜×’×•×¨×™×•×ª ×”×©×•× ×•×ª:\n"
            for category, values in graph_data.items():
                formatted_category = category.replace("future_", "").replace("_past", "").replace("_present", "")
                if "current" in values and "global" in values:
                    prompt += f"- {formatted_category}: ×¢×¨×š × ×•×›×—×™ {values['current']:.2f}, ×××•×¦×¢ ××¨×¦×™ {values['global']:.2f}\n"
    
    # ×”×•×¡×¤×ª ××™×“×¢ ×¢×œ ×‘×™×ª ×”×¡×¤×¨ ×× ×§×™×™×
    if school_info and hasattr(school_info, 'school_name'):
        prompt += f"\n×”× ×ª×•× ×™× ××ª×™×™×—×¡×™× ×œ×‘×™×ª ×”×¡×¤×¨: {school_info.school_name}"
    
    # Import the system prompt for graph explanations
    
    # Get the appropriate system prompt for graph explanations
    system_prompt = prompts.return_good_prompt()
    
    try:
        # × ×™×¡×™×•×Ÿ ×œ×§×‘×œ ×ª×©×•×‘×” ×¢× ×¡×˜×¨×™××™× ×’
        response_stream = openai_client.chat.completions.create(
            model="gpt-4o",  # ××• ××•×“×œ ××—×¨ ×©×ª×¨×¦×” ×œ×”×©×ª××© ×‘×•
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=1000,
            stream=True
        )
        
        # ×™×¦×™×¨×ª ××§×•× ×œ×”×¦×’×ª ×”×”×¡×‘×¨×™× ×‘×–××Ÿ ×××ª
        explanation_placeholder = st.empty()
        full_explanation = ""
        
        # ×œ×•×œ××” ×¢×œ ×—×œ×§×™ ×”×ª×©×•×‘×” ×‘×¡×˜×¨×™××™× ×’ ×•×”×¦×’×” ×‘×–××Ÿ ×××ª
        for chunk in response_stream:
            if chunk.choices and hasattr(chunk.choices[0], "delta") and hasattr(chunk.choices[0].delta, "content"):
                content = chunk.choices[0].delta.content
                if content:
                    full_explanation += content
                    # ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª ×¢× ×¡××Ÿ ××”×‘×”×‘
                    explanation_placeholder.markdown(full_explanation + "â–Œ")
        
        # ×”×¦×’×” ×¡×•×¤×™×ª ×©×œ ×”×”×¡×‘×¨ ×”××œ× (×œ×œ× ×¡××Ÿ)
        # explanation_placeholder.markdown(full_explanation)
        
        # ×”×—×–×¨×ª ×”×”×¡×‘×¨ ×”××œ×
        return full_explanation
        
    except Exception as e:
        # ×‘××§×¨×” ×©×œ ×©×’×™××”, ×”×—×–×¨ ×”×¡×‘×¨ ×›×œ×œ×™ ×•×”×•×“×¢×ª ×©×’×™××”
        error_msg = f"×œ× ×”×¦×œ×—× ×• ×œ×™×™×¦×¨ ×”×¡×‘×¨ ××•×˜×•××˜×™. ×©×’×™××”: {str(e)}"
        st.error(error_msg)
        
        # ×”×¡×‘×¨ ×‘×¨×™×¨×ª ××—×“×œ ×œ×¤×™ ×¡×•×’ ×”×’×¨×£
        if graph_type == "risc":
            return "×—×•×¡×Ÿ (RISC): ××•×“×“ ××ª ×”×™×›×•×œ×ª ×©×œ ×”×ª×œ××™×“×™× ×œ×”×ª××•×“×“ ×¢× ××ª×’×¨×™× ×•××¦×‘×™ ×œ×—×¥. ×¦×™×•×Ÿ ×’×‘×•×” ××¢×™×“ ×¢×œ ×™×›×•×œ×ª ×”×ª××•×“×“×•×ª ×˜×•×‘×” ×™×•×ª×¨."
        elif graph_type == "ici":
            return "××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™ (ICI): ××•×“×“ ××ª ×”×××•× ×” ×©×œ ××“× ×©×”×•× ×©×•×œ×˜ ×‘×—×™×™×• ×•×œ× ×’×•×¨××™× ×—×™×¦×•× ×™×™×. ×¦×™×•×Ÿ ×’×‘×•×” ××¢×™×“ ×¢×œ ×ª×—×•×©×ª ××¡×•×’×œ×•×ª ×¢×¦××™×ª ×—×–×§×” ×™×•×ª×¨."
        elif graph_type == "spider":
            return "×’×¨×£ ×ª×¤×™×¡×•×ª ×–××Ÿ: ××¦×™×’ ××ª ×”×—×œ×•×§×” ×©×œ ×ª×¤×™×¡×•×ª ×”×–××Ÿ ×”×©×•× ×•×ª (×¢×‘×¨ ×©×œ×™×œ×™/×—×™×•×‘×™, ×”×•×•×” ×“×˜×¨××™× ×™×¡×˜×™/×”×“×•× ×™×¡×˜×™, ×¢×ª×™×“). ××™×–×•×Ÿ × ×›×•×Ÿ ×‘×™×Ÿ ×”×ª×¤×™×¡×•×ª ×—×©×•×‘ ×œ×”×ª× ×”×’×•×ª ×‘×¨×™××” ×•×”×¦×œ×—×” ×œ×™××•×“×™×ª."
        else:
            return "×œ× ×”×¦×œ×—× ×• ×œ×™×™×¦×¨ ×”×¡×‘×¨ ××•×˜×•××˜×™ ×œ×’×¨×£ ×–×”."

# Main Dashboard
def main():
    st.title("×“××©×‘×•×¨×“ × ×™×ª×•×— × ×ª×•× ×™× ğŸ“Š")
    st.video("https://www.youtube.com/embed/o50N3-OaGdM?si=lTxJJ02igXhKWzPI", start_time=0)
    
    # Load data
    try:
        df = init()
    except Exception as e:
        st.error(f"A××™×¨×¢×” ×©×’×™××” ×‘×˜×¢×™× ×ª ×”× ×ª×•× ×™×: {e}")
        df = pd.DataFrame()  # Empty dataframe as fallback
    
    # ×‘×—×™×¨×ª ×‘×™×ª ×¡×¤×¨ ×‘×—×œ×§ ×”×¢×œ×™×•×Ÿ ×©×œ ×”×¢××•×“
    selected_school = None
    if not df.empty and 'school' in df.columns:
        # ××™×›×œ ×¢× ×¢×™×¦×•×‘ ××•×ª×× ×œ×‘×—×™×¨×ª ×‘×™×ª ×”×¡×¤×¨
        school_selector_container = st.container()
        with school_selector_container:
            st.markdown('<div class="school-selector">', unsafe_allow_html=True)
            st.markdown("<h3 style='text-align: center; margin-bottom: 15px;'>×‘×—×™×¨×ª ×‘×™×ª ×¡×¤×¨</h3>", unsafe_allow_html=True)
            unique_schools = df["school"].unique().tolist()
            selected_school = st.selectbox("×‘×—×¨ ×‘×™×ª ×¡×¤×¨:", unique_schools, key="school_selector_top")
            st.markdown('</div>', unsafe_allow_html=True)
            
        filtered_df = df[df['school'] == selected_school]
        st.session_state.graph_data["selected_school"] = selected_school
    else:
        # Demo data if no real data is available
        filtered_df = df
        st.warning("×œ× × ××¦××• × ×ª×•× ×™× ×œ×¡×™× ×•×Ÿ")
    
    # ×›×¤×ª×•×¨ "×”×¨××” ×œ×™ ×’×¨×¤×™×" ×©××¦×™×’ ××ª ×›×œ ×”×’×¨×¤×™× ×‘×‘×ª ××—×ª
    if st.button("×”×¨××” ×œ×™ ×’×¨×¤×™×", key="show_all_charts_button", type="primary", use_container_width=True):
        # ×”×¤×¢×œ×ª ×”×¤×•× ×§×¦×™×” ×©××¤×¢×™×œ×” ××ª ×›×œ ×”×’×¨×¤×™×
        show_all_charts()
    
    # ×”×•×¡×¤×ª CSS ×¡×¤×¦×™×¤×™ ×œ××•×‘×™×™×œ ×× × ×“×¨×©
    st.markdown(
        """
        <style>
        @media (max-width: 768px) {
            /* ×”×¡×ª×¨×ª ×”×¡×™×™×“×‘×¨ ×‘××•×‘×™×™×œ */
            [data-testid="stSidebar"] {
                display: none;
            }
            
            /* ×”×ª×××ª ×ª×•×›×Ÿ ×”×“×£ ×œ××œ×•× ×¨×•×—×‘ ×”××¡×š */
            .main .block-container {
                padding-left: 1rem;
                padding-right: 1rem;
            }
        }
        </style>
        """,
        unsafe_allow_html=True
    )
    
    # ×‘×“×™×§×” ××•×‘×™×™×œ - ×× ×¨×•×—×‘ ×”××¡×š ×§×˜×Ÿ ×-768px, × ×¦×™×’ ××ª ×”×’×¨×¤×™× ××—×“ ××ª×—×ª ×œ×©× ×™
    # Initialize mobile detection in session state if not already set
    if "is_mobile" not in st.session_state:
        st.session_state.is_mobile = False
        
    # # Option 1: Allow manual toggle for mobile view in sidebar
    # with st.sidebar:
    #     st.session_state.is_mobile = st.checkbox("×ª×¦×•×’×ª ××•×‘×™×™×œ", value=st.session_state.is_mobile)
    
    is_mobile = True #st.session_state.is_mobile
    
    if is_mobile:
        # ×‘××•×‘×™×™×œ - ×›×œ ×’×¨×£ ×‘×§×•×œ×•× ×” × ×¤×¨×“×ª ××—×“ ××ª×—×ª ×œ×©× ×™
        st.markdown("<p style='font-size:0.8rem;color:#666;'>×ª×¦×•×’×ª ××•×‘×™×™×œ ××•×¤×¢×œ×ª</p>", unsafe_allow_html=True)
        row1_col1 = st.container()
        row1_col2 = st.container()
        row1_col3 = st.container()
    else:
        # ×‘×“×¡×§×˜×•×¤ - ×©×œ×•×©×” ×’×¨×¤×™× ×‘×©×•×¨×”
        row1_col1, row1_col2, row1_col3 = st.columns(3)
    
    # Generate sample graphs if no data
    if filtered_df.empty:
        # Sample data for demonstration
        with row1_col1:
            st.markdown("### ××“×“ ×—×•×¡×Ÿ")
            
            # ×›×¤×ª×•×¨ ×œ×”×¦×’×ª/×”×¡×ª×¨×ª ×”×’×¨×£
            if st.button("×”×¦×’ ×œ×™ ××ª ×”×’×¨×£", key="show_sample_risc", type="primary"):
                st.session_state.show_charts["sample_risc"] = not st.session_state.show_charts["sample_risc"]
            
            # ×”×¦×’×ª ×”×’×¨×£ ×¨×§ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
            if st.session_state.show_charts["sample_risc"]:
                fig1 = generate_sample_gauge()
                st.plotly_chart(fig1, use_container_width=True)
                
                # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×œ×“×•×’××” ×œ×¦'××˜×‘×•×˜
                st.session_state.graph_data["risc"] = {
                    "value": 3.7,
                    "global_avg": 3.5,
                    "research_avg": 3.8
                }
                
        with row1_col2:
            st.markdown("### ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™")
            
            # ×›×¤×ª×•×¨ ×œ×”×¦×’×ª/×”×¡×ª×¨×ª ×”×’×¨×£
            if st.button("×”×¦×’ ×œ×™ ××ª ×”×’×¨×£", key="show_sample_ici", type="primary"):
                st.session_state.show_charts["sample_ici"] = not st.session_state.show_charts["sample_ici"]
            
            # ×”×¦×’×ª ×”×’×¨×£ ×¨×§ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
            if st.session_state.show_charts["sample_ici"]:
                fig2 = generate_sample_gauge(value=3.4, title="××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™")
                st.plotly_chart(fig2, use_container_width=True)
                
                # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×œ×“×•×’××” ×œ×¦'××˜×‘×•×˜
                st.session_state.graph_data["ici"] = {
                    "value": 3.4,
                    "global_avg": 3.2,
                    "research_avg": 3.6
                }
                
        with row1_col3:
            st.markdown("### × ×ª×•× ×™ ×¦×™×¨ ×”×–××Ÿ")
            
            # ×›×¤×ª×•×¨ ×œ×”×¦×’×ª/×”×¡×ª×¨×ª ×”×’×¨×£
            if st.button("×”×¦×’ ×œ×™ ××ª ×”×’×¨×£", key="show_sample_spider", type="primary"):
                st.session_state.show_charts["sample_spider"] = not st.session_state.show_charts["sample_spider"]
            
            # ×”×¦×’×ª ×”×’×¨×£ ×¨×§ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
            if st.session_state.show_charts["sample_spider"]:
                fig3 = generate_sample_spider()
                st.plotly_chart(fig3, use_container_width=True)
                
                # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×œ×“×•×’××” ×œ×¦'××˜×‘×•×˜
                st.session_state.graph_data["spider"] = {
                    "negetive_past": {"current": 3.2, "global": 3.0, "research": 2.8},
                    "positive_past": {"current": 4.1, "global": 3.8, "research": 4.0},
                    "fatalic_present": {"current": 2.8, "global": 3.0, "research": 2.5},
                    "hedonistic_present": {"current": 3.5, "global": 3.2, "research": 3.0},
                    "future": {"current": 3.9, "global": 3.7, "research": 4.0},
                }
                
                # ×›×¤×ª×•×¨ "××” ×–×” ××•××¨?" ×¢×‘×•×¨ ×’×¨×£ ×ª×¤×™×¡×•×ª ×–××Ÿ
                if st.button("××” ×–×” ××•××¨?", key="explain_spider"):
                    # ×—×•×¡× ××ª ×”×¦×’×ª ×”×”×¡×‘×¨ - ×¤×©×•×˜ ×œ× ×¢×•×©×” ×›×œ×•×
                    pass
    else:
        # Real graphs from data
        # try:
        #     school_info = SchoolInfo(filtered_df)
            
        #     with row1_col1:
        #         st.markdown("### ××“×“ ×—×•×¡×Ÿ")
                
        #         # ×›×¤×ª×•×¨ ×œ×”×¦×’×ª/×”×¡×ª×¨×ª ×”×’×¨×£
        #         if st.button("×”×¦×’ ×œ×™ ××ª ×”×’×¨×£", key="show_risc", type="primary"):
        #             st.session_state.show_charts["risc"] = not st.session_state.show_charts["risc"]
                
        #         # ×”×¦×’×ª ×”×’×¨×£ ×¨×§ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
        #         if st.session_state.show_charts["risc"]:
        #             fig_risc = school_info.get_fig_risc("×—×•×¡×Ÿ")
        #             st.plotly_chart(fig_risc, use_container_width=True)
                
        #             # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×××™×ª×™×™× ×œ×¦'××˜×‘×•×˜
        #             st.session_state.graph_data["risc"] = {
        #                 "value": school_info.risc,
        #                 "global_avg": st.session_state.global_average["risc"],
        #                 "research_avg": st.session_state.research_average["risc"]
        #             }
                
        #     with row1_col2:
        #         st.markdown("### ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™")
                
        #         # ×›×¤×ª×•×¨ ×œ×”×¦×’×ª/×”×¡×ª×¨×ª ×”×’×¨×£
        #         if st.button("×”×¦×’ ×œ×™ ××ª ×”×’×¨×£", key="show_ici", type="primary"):
        #             st.session_state.show_charts["ici"] = not st.session_state.show_charts["ici"]
                
        #         # ×”×¦×’×ª ×”×’×¨×£ ×¨×§ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
        #         if st.session_state.show_charts["ici"]:
        #             fig_ici = school_info.get_fig_ici("××™×§×•×“ ×©×œ×™×˜×”")
        #             st.plotly_chart(fig_ici, use_container_width=True)
                
        #             # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×××™×ª×™×™× ×œ×¦'××˜×‘×•×˜
        #             st.session_state.graph_data["ici"] = {
        #                 "value": school_info.ici,
        #                 "global_avg": st.session_state.global_average["ici"],
        #                 "research_avg": st.session_state.research_average["ici"]
        #             }
                
        #     with row1_col3:
        #         st.markdown("### ×”×ª×¤×œ×’×•×ª ×œ×¤×™ ×××“×™ ×–××Ÿ")
                
        #         # ×›×¤×ª×•×¨ ×œ×”×¦×’×ª/×”×¡×ª×¨×ª ×”×’×¨×£
        #         if st.button("×”×¦×’ ×œ×™ ××ª ×”×’×¨×£", key="show_spider", type="primary"):
        #             st.session_state.show_charts["spider"] = not st.session_state.show_charts["spider"]
                
        #         # ×”×¦×’×ª ×”×’×¨×£ ×¨×§ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
        #         if st.session_state.show_charts["spider"]:
        #             fig_spider = school_info.get_fig_spider(mobile=True)
        #             st.plotly_chart(fig_spider, use_container_width=True)
                
        #             # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×××™×ª×™×™× ×œ×¦'××˜×‘×•×˜
        #             anigmas_dict = school_info.return_anigmas_result_as_dict()
        #             st.session_state.graph_data["spider"] = {
        #                 "future_negetive_past": {
        #                     "current": anigmas_dict["future_negetive_past"],
        #                     "global": st.session_state.global_average["future_negetive_past"],
        #                     "research": st.session_state.research_average["future_negetive_past"]
        #                 },
        #                 "future_positive_past": {
        #                     "current": anigmas_dict["future_positive_past"],
        #                     "global": st.session_state.global_average["future_positive_past"],
        #                     "research": st.session_state.research_average["future_positive_past"]
        #                 },
        #                 "future_fatalic_present": {
        #                     "current": anigmas_dict["future_fatalic_present"],
        #                     "global": st.session_state.global_average["future_fatalic_present"],
        #                     "research": st.session_state.research_average["future_fatalic_present"]
        #                 },
        #                 "future_hedonistic_present": {
        #                     "current": anigmas_dict["future_hedonistic_present"],
        #                     "global": st.session_state.global_average["future_hedonistic_present"],
        #                     "research": st.session_state.research_average["future_hedonistic_present"]
        #                 },
        #                 "future_future": {
        #                     "current": anigmas_dict["future_future"],
        #                     "global": st.session_state.global_average["future_future"],
        #                     "research": st.session_state.research_average["future_future"]
        #                 }
        #             }
            
            # Divider - ×”×•×¢×‘×¨ ×œ××§×•× ×”×–×” ×©×”×•× ××ª×—×ª ×œ×›×œ ×”×’×¨×¤×™× ×•×”×›×¤×ª×•×¨×™×
            st.markdown("---")
            
            # Chatbot section with API key information
            st.markdown("### ×¦'××˜×‘×•×˜ ×œ× ×™×ª×•×— × ×ª×•× ×™× ğŸ¤–")
            
            if 'school_info' in locals():
                short = llm_gpt.return_llm_answer(prompts.return_highlighted_text(school_info), "", "")
                # Show API status
                if api_key:
                    st.success(short)
                else:
                    st.warning("×œ× × ××¦× API key ×©×œ OpenAI. ×”×¦'××˜×‘×•×˜ ×¢×©×•×™ ×œ× ×œ×¤×¢×•×œ ×›×¨××•×™.")
            else:
                # Handle case when school_info is not defined
                if api_key:
                    st.success("×”×¦'××˜×‘×•×˜ ××•×›×Ÿ ×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª")
                else:
                    st.warning("×œ× × ××¦× API key ×©×œ OpenAI. ×”×¦'××˜×‘×•×˜ ×¢×©×•×™ ×œ× ×œ×¤×¢×•×œ ×›×¨××•×™.")
            
            st.markdown("×©××œ ×©××œ×•×ª ×¢×œ ×”× ×ª×•× ×™× ×”××•×¦×’×™× ×‘×’×¨×¤×™× ××• ×¢×œ ×”×ª×•×¦××•×ª ×”×›×œ×œ×™×•×ª")
            
            # ×”×¦×¢×•×ª ×œ×©××œ×•×ª ×œ××©×ª××©
            suggested_questions = [
                "×”×›×Ÿ ×“×•×— ×× ×”×œ",
            ]
            
            # ×××©×§ ××©×ª××© ×©××¨××” ×”×¦×¢×•×ª ×œ×©××œ×•×ª
            st.markdown("#### ×©××œ×•×ª ×œ×“×•×’××”:")
            cols = st.columns(3)
            for i, question in enumerate(suggested_questions):
                col_idx = i % 3
                if cols[col_idx].button(question, key=f"question_{i}"):
                    # Add user message to chat history
                    st.session_state.messages.append({"role": "user", "content": question})
                    
                    # ××¦×™×’×™× ××ª ×©××œ×ª ×”××©×ª××©
                    with st.chat_message("user"):
                        st.markdown(question)
                        
                    # ××¦×™×’×™× ××ª ×ª×©×•×‘×ª ×”×¦'××˜×‘×•×˜ ×¢× ×¡×˜×¨×™××™× ×’ ×‘×–××Ÿ ×××ª
                    with st.chat_message("assistant"):
                        data = filtered_df.to_markdown() if not filtered_df.empty else "××™×Ÿ × ×ª×•× ×™× ×–××™× ×™×"
                        #system_prompt = prompts.return_good_prompt() if 'school_info' not in locals() else prompts.return_good_prompt(school_info)
                        
                        # ×©×™××•×© ×‘×¡×˜×¨×™××™× ×’ ×¢×‘×•×¨ ×©××œ×•×ª ×“×•×’××”
                        response_stream = get_openai_response(
                            question, 
                            system_prompt, 
                            st.session_state.messages[:-1], 
                            st.session_state.graph_data,
                            stream=True
                        )
                        
                        # ××§×•× ×œ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª
                        message_placeholder = st.empty()
                        full_response = ""
                        
                        # ×œ×•×œ××” ×¢×œ ××™×¨×•×¢×™ ×”×¡×˜×¨×™× ×-OpenAI ×•×”×¦×’×ª× ×‘×–××Ÿ ×××ª
                        for chunk in response_stream:
                            if chunk.choices and hasattr(chunk.choices[0], "delta") and hasattr(chunk.choices[0].delta, "content"):
                                content = chunk.choices[0].delta.content
                                if content:
                                    full_response += content
                                    # ××¦×™×’ ××ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª ×¢× ×¡××Ÿ ××”×‘×”×‘
                                    message_placeholder.markdown(full_response + "â–Œ")
                        
                        # ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××œ××” ×”×¡×•×¤×™×ª (×œ×œ× ×”×¡××Ÿ)
                        message_placeholder.markdown(full_response)
                    
                    # ×”×•×¡×¤×ª ×”×ª×©×•×‘×” ×œ×”×™×¡×˜×•×¨×™×”
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
        
    # except Exception as e:
    #         st.error(f"×©×’×™××” ×‘×¢×ª ×™×¦×™×¨×ª ×”×’×¨×¤×™×: {e}")
    
    # ×”×¦×’×ª ×”×™×¡×˜×•×¨×™×™×ª ×¦'××˜ (×¨×§ ××”×™×¡×˜×•×¨×™×” ×§×•×“××ª)
    messages_to_display = st.session_state.messages[:-2] if len(st.session_state.messages) >= 2 else []
    if messages_to_display:
        st.markdown("#### ×”×™×¡×˜×•×¨×™×™×ª ×¦'××˜:")
        chat_container = st.container()
        with chat_container:
            for message in messages_to_display:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
    
    # Input for chat
    if prompt := st.chat_input("×©××œ ×©××œ×” ×¢×œ ×”× ×ª×•× ×™×..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # ××¦×™×’×™× ××ª ×©××œ×ª ×”××©×ª××©
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ××¦×™×’×™× ××ª ×ª×©×•×‘×ª ×”×¦'××˜×‘×•×˜ ×¢× ×¡×˜×¨×™××™× ×’ ×‘×–××Ÿ ×××ª
        with st.chat_message("assistant"):
            # ××§×•× ×œ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª
            message_placeholder = st.empty()
            full_response = ""
            
            # ×§×‘×œ×ª ×ª×©×•×‘×” ××”×¦'××˜×‘×•×˜ ×¢× ×¡×˜×¨×™××™× ×’
            response_stream = get_openai_response(
                prompt, 
                llm_system_massage_manager.get_first_system_prompt(filtered_df.to_markdown() if not filtered_df.empty else "××™×Ÿ × ×ª×•× ×™× ×–××™× ×™×"), 
                st.session_state.messages[:-1], 
                st.session_state.graph_data,
                stream=True
            )
            
            # ×œ×•×œ××” ×¢×œ ××™×¨×•×¢×™ ×”×¡×˜×¨×™× ×•×”×¦×’×ª ×”×ª×©×•×‘×” ×‘×–××Ÿ ×××ª
            for chunk in response_stream:
                if chunk.choices and hasattr(chunk.choices[0], "delta") and hasattr(chunk.choices[0].delta, "content"):
                    content = chunk.choices[0].delta.content
                    if content:
                        full_response += content
                        # ××¦×™×’ ××ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª ×¢× ×¡××Ÿ ××”×‘×”×‘
                        message_placeholder.markdown(full_response + "â–Œ")
            
            # ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××œ××” ×”×¡×•×¤×™×ª (×œ×œ× ×”×¡××Ÿ)
            message_placeholder.markdown(full_response)
        
        # ×”×•×¡×¤×ª ×”×ª×©×•×‘×” ×œ×”×™×¡×˜×•×¨×™×”
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        
        # ×‘×“×™×§×ª × ×ª×•× ×™× ×•×”×•×¡×¤×ª ×¡×™×›×•× ×’×¨×¤×™× ×‘×ª×—×™×œ×ª ×”×©×™×—×” ×¢× ×”×¦'××˜×‘×•×˜
        all_charts_displayed = any([
            st.session_state.show_charts["risc"] or st.session_state.show_charts["sample_risc"],
            st.session_state.show_charts["ici"] or st.session_state.show_charts["sample_ici"],
            st.session_state.show_charts["spider"] or st.session_state.show_charts["sample_spider"]
        ])

        # ×× ××•×¦×’×™× ×’×¨×¤×™× ××‘×œ ×¢×“×™×™×Ÿ ×œ× × ×•×¡×£ ×”×¡×™×›×•× ×œ×¦'××˜
        if all_charts_displayed and not st.session_state.summary_added_to_chat and len(st.session_state.messages) == 0:
            # Generate graph summary
            summary_prompt = "×¡×›× ××ª ×›×œ ×”×’×¨×¤×™× ×”××•×¦×’×™× ×•×ª×Ÿ ×œ×™ ×”×¡×‘×¨ ×›×œ×œ×™ ×©×œ ××” ×”× ××¨××™×. ×ª×Ÿ ×œ×–×” ×›×•×ª×¨×ª '××” ×–×” ××•××¨ - ×¡×™×›×•× ×”×’×¨×¤×™×'"
            
            # ×™×¦×™×¨×ª ×”×¡×‘×¨ ××¡×›× ×œ×›×œ ×”×’×¨×¤×™×
            with st.spinner('××›×™×Ÿ ×¡×™×›×•× ×©×œ ×”×’×¨×¤×™×...'):
                data = filtered_df.to_markdown() if not filtered_df.empty else "××™×Ÿ × ×ª×•× ×™× ×–××™× ×™×"
                system_prompt = prompts.return_good_prompt() if 'school_info' not in locals() else prompts.return_good_prompt(school_info)
                
                # ×§×‘×œ×ª ×ª×©×•×‘×” ××”××•×“×œ
                summary_response = get_openai_response(
                    summary_prompt,
                    system_prompt,
                    [],  # ××™×Ÿ ×”×™×¡×˜×•×¨×™×” ×§×•×“××ª
                    st.session_state.graph_data,
                    stream=False
                )
                
                # ×”×•×¡×¤×ª ×”×”×•×“×¢×” ×œ×¦'××˜×‘×•×˜
                st.session_state.messages.append({"role": "assistant", "content": summary_response})
                st.session_state.summary_added_to_chat = True

# Helper functions for sample graphs if no data is available
def generate_sample_gauge(value=3.7, title="×—×•×¡×Ÿ"):
    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=value,
        title={'text': title, 'font': {'size': 24}},
        gauge={
            'axis': {'range': [1, 5]},
            'bar': {'color': "#437742"},
            'steps': [
                {'range': [1, 2], 'color': "#f4f4f4"},
                {'range': [2, 3], 'color': "#f0f0f0"},
                {'range': [3, 4], 'color': "#e8e8e8"},
                {'range': [4, 5], 'color': "#e0e0e0"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 3.5
            }
        }
    ))
    fig.update_layout(height=300)
    return None#ull#fig

def generate_sample_bar_chart():
    categories = ["×§×˜×’×•×¨×™×” ×", "×§×˜×’×•×¨×™×” ×‘", "×§×˜×’×•×¨×™×” ×’", "×§×˜×’×•×¨×™×” ×“"]
    values = [4.2, 3.8, 2.5, 3.9]
    
    df = pd.DataFrame({"name": categories, "value": values})
    fig = px.bar(df, x='name', y='value', title="×”×ª×¤×œ×’×•×ª ×¦×™×•× ×™×")
    
    # Add reference lines
    fig.add_hline(y=3.5, line=dict(color="#F37321"), annotation_text="×××•×¦×¢ ××—×§×¨×™")
    fig.add_hline(y=3.2, line=dict(color="#1F3B91"), annotation_text="×××•×¦×¢ ××¨×¦×™")
    
    fig.update_layout(height=300)
    return None#fig

def generate_sample_spider():
    categories = ["×ª×¤×™×¡×ª ×¢×‘×¨ ××¢×›×‘×ª", "×¢×‘×¨ ×›×ª×©×ª×™×ª ×—×™×•×‘×™×ª", "×“×˜×¨××™× ×¡×˜×™×•×ª",
                 "×¡×™×¤×•×§ ××™×™×“×™", "×¢×ª×™×“"]
    
    fig = go.Figure()
    
    # Current values
    fig.add_trace(go.Scatterpolar(
        r=[3.2, 4.1, 2.8, 3.5, 3.9],
        theta=categories,
        fill='none',
        name='×××•×¦×¢ × ×•×›×—×™',
        line=dict(color='#437742')
    ))
    
    # Global average
    fig.add_trace(go.Scatterpolar(
        r=[3.0, 3.8, 3.0, 3.2, 3.7],
        theta=categories,
        fill='none',
        name='×××•×¦×¢ ××¨×¦×™',
        line=dict(color='#1F3B91')
    ))
    
    # Research average
    fig.add_trace(go.Scatterpolar(
        r=[2.8, 4.0, 2.5, 3.0, 4.0],
        theta=categories,
        fill='none',
        name='×××•×¦×¢ ××—×§×¨×™',
        line=dict(color='#F37321')
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[1, 5]
            )
        ),
        showlegend=True,
        height=300
    )
    
    return None#fig

# Run the main dashboard
if __name__ == "__main__":
    main()