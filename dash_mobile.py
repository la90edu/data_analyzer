import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import time
import os
from dotenv import load_dotenv
from openai import OpenAI

# Import custom modules
import llm_system_massage_manager
import llm_gpt
import draw_gauge
import draw_spider_graph
import draw_bar_chart
import connect_to_google_sheet
import consts
import anigmas
from class_school_info import SchoolInfo
from graph_manager import Gauge_Graph_type, Spider_Graph_type, Bar_Chart_Graph_type
from system_prompt import return_prompt,return_highlighted_text
# ×˜×¢×™× ×ª ××©×ª× ×™ ×”×¡×‘×™×‘×” ××§×•×‘×¥ .env
load_dotenv()

# ×§×‘×œ×ª ×”-API key ××”××©×ª× ×™× ×•×”×’×“×¨×ª ×”×œ×§×•×—
api_key = os.getenv("OPENAI_API_KEY")
if api_key:
    openai_client = OpenAI(api_key=api_key)
else:
    openai_client = OpenAI()  # ×™×ª×›×Ÿ ×©×™×¢×‘×•×“ ×× ×™×© API key ×‘×¡×‘×™×‘×”

# Page Configuration
st.set_page_config(
    page_title="×“××©×‘×•×¨×“ × ×™×ª×•×— × ×ª×•× ×™×",
    page_icon="ğŸ“Š",
    layout="wide",
)

# Add RTL support and custom styling
st.markdown(
    """
    <style>
    /* RTL Support */
    h1, h2, h3, h4, h5, h6, p, div {
        text-align: right;
        direction: rtl;
    }
    
    /* Sidebar open by default */
    [data-testid="stSidebar"][aria-expanded="true"] {
        width: 300px !important;
    }
    
    [data-testid="stSidebar"][aria-expanded="false"] {
        width: 300px !important;
        margin-left: -300px;
    }
    
    /* Force sidebar to be open on page load */
    section[data-testid="stSidebar"] {
        display: block !important;
        opacity: 1 !important;
        transform: none !important;
    }
    
    /* Dashboard Layout Styling */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    
    /* Card styling for graphs */
    .stPlotlyChart {
        background-color: white;
        border-radius: 10px;
        padding: 10px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin-bottom: 1rem;
    }
    
    /* Chat container styling */
    .chat-container {
        background-color: white;
        border-radius: 10px;
        padding: 15px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        height: 100%;
    }
    
    /* ×©×™×¤×•×¨ ×’×•×“×œ ×”×¢××•×“×•×ª ×œ×× ×™×¢×ª ×“×—×™×¡×” ×©×œ ×”×’×¨×¤×™× */
    .row-widget.stButton, [data-testid="stVerticalBlock"] > div {
        padding: 0 !important;
        margin: 0 !important;
    }
    
    /* ×•×™×“×•× ×©×”×’×¨×¤×™× ××§×‘×œ×™× ××ª ×”×¨×•×—×‘ ×”××ª××™× */
    [data-testid="column"] {
        width: auto !important;
        min-width: 0;
        flex: 1;
    }
    
    /* ×¢×™×¦×•×‘ ×¨×›×™×‘ ×‘×—×™×¨×ª ×‘×™×ª ×¡×¤×¨ ×‘×—×œ×§ ×”×¢×œ×™×•×Ÿ */
    .school-selector {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 10px;
        margin-bottom: 20px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
    }
    
    .school-selector .stSelectbox {
        max-width: 400px;
        margin: 0 auto;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Initialize data
def init():
    data = connect_to_google_sheet.return_data()
    df = pd.DataFrame(data)
    consts.init_st_session_global_average_and_research_average(df)
    consts.init_heg_avg(df)
    return df

#knowledge_files=

# Initialize session state for chat and graph data
if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "graph_data" not in st.session_state:
    st.session_state.graph_data = {
        "risc": None,
        "ici": None, 
        "spider": None,
        "selected_school": None
    }

# Initialize session state for graph explanations
if "graph_explanations" not in st.session_state:
    st.session_state.graph_explanations = {
        "risc": {"show": False, "explanation": ""},
        "ici": {"show": False, "explanation": ""},
        "spider": {"show": False, "explanation": ""}
    }

# ×”×¤×•× ×§×¦×™×” ×”××©×•×¤×¨×ª ×œ×§×‘×œ×ª ×ª×©×•×‘×” ×-OpenAI ×¢× ×ª××™×›×” ×‘× ×ª×•× ×™ ×”×’×¨×¤×™× ×•×‘×¡×˜×¨×™××™× ×’
def get_openai_response(prompt, system_prompt, history, graph_data, stream=False):
    # ×™×¦×™×¨×ª ×ª×§×¦×™×¨ ×©×œ × ×ª×•× ×™ ×”×’×¨×¤×™× ×›×“×™ ×œ×”×¢×‘×™×¨ ××•×ª× ×œ×¦'××˜×‘×•×˜
    graph_summary = ""
    if graph_data["selected_school"]:
        graph_summary += f"×”× ×ª×•× ×™× ×©×œ×”×œ×Ÿ ××ª×™×™×—×¡×™× ×œ×‘×™×ª ×”×¡×¤×¨: {graph_data['selected_school']}\n\n"
    
    if graph_data["risc"] is not None:
        graph_summary += f"× ×ª×•× ×™ ×’×¨×£ ×—×•×¡×Ÿ :\n"
        graph_summary += f"×¢×¨×š × ×•×›×—×™: {graph_data['risc']['value']:.2f}\n"
        graph_summary += f"×××•×¦×¢ ××¨×¦×™: {graph_data['risc']['global_avg']:.2f}\n"
        #graph_summary += f"×××•×¦×¢ ××—×§×¨×™: {graph_data['risc']['research_avg']:.2f}\n\n"
        
    if graph_data["ici"] is not None:
        graph_summary += f"× ×ª×•× ×™ ×’×¨×£ ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™ (ICI):\n"
        graph_summary += f"×¢×¨×š × ×•×›×—×™: {graph_data['ici']['value']:.2f}\n"
        graph_summary += f"×××•×¦×¢ ××¨×¦×™: {graph_data['ici']['global_avg']:.2f}\n"
       # graph_summary += f"×××•×¦×¢ ××—×§×¨×™: {graph_data['ici']['research_avg']:.2f}\n\n"
        
    if graph_data["spider"] is not None:
        graph_summary += f"×œ×ª×¤×™×¡×•×ª ×–××Ÿ:\n"
        
        for category, values in graph_data["spider"].items():
            formatted_category = category.replace("future_", "").replace("_past", "").replace("_present", "")
            graph_summary += f"×§×˜×’×•×¨×™×”: {formatted_category}\n"
            graph_summary += f"  ×¢×¨×š × ×•×›×—×™: {values['current']:.2f}\n"
            graph_summary += f"  ×××•×¦×¢ ××¨×¦×™: {values['global']:.2f}\n"
            #graph_summary += f"  ×××•×¦×¢ ××—×§×¨×™: {values['research']:.2f}\n"
    
    # ×”×•×¡×¤×ª ×”× ×—×™×•×ª ×¡×¤×¦×™×¤×™×•×ª ×œ×’×¨×¤×™× ×‘×¤×¨×•××¤×˜ ×”××¢×¨×›×ª
    graph_system_prompt = system_prompt + ""

    
    # ×”×•×¡×¤×ª ×ª×§×¦×™×¨ ×”×’×¨×¤×™× ×œ×¤×¨×•××¤×˜ ×”××©×ª××©
    user_prompt = f"""history: {history}

current prompt: {prompt}

× ×ª×•× ×™ ×”×’×¨×¤×™× ×”××•×¦×’×™× ×‘×“××©×‘×•×¨×“:
{graph_summary}
"""
    
    try:
        if stream:
            # ×”×—×–×¨ ××•×‘×™×™×§×˜ ×¡×˜×¨×™××™× ×’ ×©× ×™×ª×Ÿ ×œ××™×˜×¨×¦×™×”
            response_stream = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": graph_system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.05,  # ×”×•×¡×¤×ª ×˜××¤×¨×˜×•×¨×” × ××•×›×” ×œ×ª×©×•×‘×•×ª ×™×•×ª×¨ ×××•×§×“×•×ª
                stream=True
            )
            return response_stream
        else:
            # ×”×—×–×¨×ª ×ª×©×•×‘×” ××œ××” (×œ×œ× ×¡×˜×¨×™××™× ×’)
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": graph_system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.05  # ×”×•×¡×¤×ª ×˜××¤×¨×˜×•×¨×” × ××•×›×” ×œ×ª×©×•×‘×•×ª ×™×•×ª×¨ ×××•×§×“×•×ª
            )
            return response.choices[0].message.content
    except Exception as e:
        return f"××™×¨×¢×” ×©×’×™××” ×‘×¢×ª ×”×ª×§×©×•×¨×ª ×¢× OpenAI: {str(e)}"

# Streamed response generator for chatbot
def response_generator(prompt, df, graph_data,school_info):
    data = df.to_markdown()
    system_prompt = return_prompt(school_info) #llm_system_massage_manager.get_first_system_prompt(data)
    
    # ×§×‘×œ×ª ×”×ª×©×•×‘×” ×‘×××¦×¢×•×ª ×”-API Key ××§×•×‘×¥ .env ×•×”×¢×‘×¨×ª × ×ª×•× ×™ ×”×’×¨×¤×™×
    # ×¢× ×¡×˜×¨×™××™× ×’ ××•×¤×¢×œ
    response_stream = get_openai_response(prompt, system_prompt, st.session_state.messages, graph_data, stream=True)
    
    full_response = ""
    # ×œ×•×œ××” ×¢×œ ××™×¨×•×¢×™ ×”×¡×˜×¨×™× ×-OpenAI
    for chunk in response_stream:
        if chunk.choices and hasattr(chunk.choices[0], "delta") and hasattr(chunk.choices[0].delta, "content"):
            content = chunk.choices[0].delta.content
            if content:
                full_response += content
                yield content  # ×©×œ×™×—×ª ×›×œ ×—×œ×§ ×©××ª×§×‘×œ ××™×“×™×ª ×œ×ª×¦×•×’×”
    
    # ×”×—×–×¨×ª ×”×ª×©×•×‘×” ×”××œ××” ×œ×¦×•×¨×š ×©××™×¨×” ×‘×”×™×¡×˜×•×¨×™×”
    return full_response

# ×¤×•× ×§×¦×™×” ×œ×§×‘×œ×ª ×”×¡×‘×¨ ×¢×œ ×’×¨×£ ×¡×¤×¦×™×¤×™
def get_graph_explanation(graph_type, graph_data, school_info=None):
    """
    ××§×‘×œ×ª ×”×¡×‘×¨ ×¢×œ ×’×¨×£ ×¡×¤×¦×™×¤×™ ×××•×“×œ ×”×©×¤×”
    
    Args:
        graph_type: ×¡×•×’ ×”×’×¨×£ (risc, ici, spider)
        graph_data: ×”× ×ª×•× ×™× ×©×œ ×”×’×¨×£
        school_info: ××•×‘×™×™×§×˜ SchoolInfo ×× ×§×™×™×
    
    Returns:
        ×”×¡×‘×¨ ×˜×§×¡×˜×•××œ×™ ××¤×•×¨×˜ ×¢×œ ×”×’×¨×£ ×•××©××¢×•×ª×•
    """
    if graph_type == "risc":
        prompt = f"""
        ×”×¡×‘×¨ ×‘×‘×§×©×” ××ª × ×ª×•× ×™ ×’×¨×£ ×”×—×•×¡×Ÿ ×”×‘××™×:
        ×¢×¨×š × ×•×›×—×™: {graph_data['value']:.2f}
        ×××•×¦×¢ ××¨×¦×™: {graph_data['global_avg']:.2f}
        
        ××” ×”××©××¢×•×ª ×©×œ ×”× ×ª×•× ×™× ×”××œ×”? ×”×× ×”×¢×¨×š ×’×‘×•×” ××• × ××•×š ×‘×™×—×¡ ×œ×××•×¦×¢? ××” ×–×” ××•××¨ ×¢×œ ×‘×™×ª ×”×¡×¤×¨?
        ×”×ª×™×™×—×¡ ×œ×”×©×œ×›×•×ª ×”×—×™× ×•×›×™×•×ª ×•×”×—×‘×¨×ª×™×•×ª ×©×œ ×”×ª×•×¦××•×ª. ×ª×Ÿ ×”××œ×¦×•×ª ×œ×”××©×š ×‘×”×ª×‘×¡×¡ ×¢×œ ×”× ×ª×•× ×™×.
        """
    elif graph_type == "ici":
        prompt = f"""
        ×”×¡×‘×¨ ×‘×‘×§×©×” ××ª × ×ª×•× ×™ ×’×¨×£ ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™ (ICI) ×”×‘××™×:
        ×¢×¨×š × ×•×›×—×™: {graph_data['value']:.2f}
        ×××•×¦×¢ ××¨×¦×™: {graph_data['global_avg']:.2f}
        
        ××” ×”××©××¢×•×ª ×©×œ ×”× ×ª×•× ×™× ×”××œ×”? ×”×× ×”×¢×¨×š ×’×‘×•×” ××• × ××•×š ×‘×™×—×¡ ×œ×××•×¦×¢? ××” ×–×” ××•××¨ ×¢×œ ×‘×™×ª ×”×¡×¤×¨?
        ×”×ª×™×™×—×¡ ×œ×”×©×œ×›×•×ª ×”×—×™× ×•×›×™×•×ª ×•×”×—×‘×¨×ª×™×•×ª ×©×œ ×”×ª×•×¦××•×ª. ×ª×Ÿ ×”××œ×¦×•×ª ×œ×”××©×š ×‘×”×ª×‘×¡×¡ ×¢×œ ×”× ×ª×•× ×™×.
        """
    elif graph_type == "spider":
        prompt = """
        ×”×¡×‘×¨ ×‘×‘×§×©×” ××ª × ×ª×•× ×™ ×’×¨×£ ×ª×¤×™×¡×•×ª ×”×–××Ÿ (×’×¨×£ ×”×¢×›×‘×™×©) ×”××•×¦×’.
        ××”×™ ×”××©××¢×•×ª ×©×œ ×›×œ ××—×“ ××”×××“×™×? ××™×œ×• ×××“×™× ×‘×•×œ×˜×™× ×œ×˜×•×‘×” ×•××™×œ×• ×œ×¨×¢×”?
        ××”×Ÿ ×”×”×©×œ×›×•×ª ×”×—×™× ×•×›×™×•×ª ×•×”×¤×¡×™×›×•×œ×•×’×™×•×ª ×©×œ ×”×ª×•×¦××•×ª?
        ×ª×Ÿ ×”××œ×¦×•×ª ×œ×”××©×š ×‘×”×ª×‘×¡×¡ ×¢×œ ×”× ×ª×•× ×™×.
        """
        
        # ×”×•×¡×¤×ª ×¤×™×¨×•×˜ ×¢×œ ×”×§×˜×’×•×¨×™×•×ª ×‘×’×¨×£ ×”×¢×›×‘×™×© ×× ×™×© × ×ª×•× ×™×
        if graph_data:
            prompt += "\n\n× ×ª×•× ×™ ×”×§×˜×’×•×¨×™×•×ª ×”×©×•× ×•×ª:\n"
            for category, values in graph_data.items():
                formatted_category = category.replace("future_", "").replace("_past", "").replace("_present", "")
                if "current" in values and "global" in values:
                    prompt += f"- {formatted_category}: ×¢×¨×š × ×•×›×—×™ {values['current']:.2f}, ×××•×¦×¢ ××¨×¦×™ {values['global']:.2f}\n"
    
    # ×”×•×¡×¤×ª ××™×“×¢ ×¢×œ ×‘×™×ª ×”×¡×¤×¨ ×× ×§×™×™×
    if school_info and hasattr(school_info, 'school_name'):
        prompt += f"\n×”× ×ª×•× ×™× ××ª×™×™×—×¡×™× ×œ×‘×™×ª ×”×¡×¤×¨: {school_info.school_name}"
    
    try:
        system_prompt = """××ª×” ×™×•×¢×¥ ×—×™× ×•×›×™ ××•××—×” ×‘× ×™×ª×•×— × ×ª×•× ×™× ×¤×¡×™×›×•×œ×•×’×™×™× ×©×œ ×ª×œ××™×“×™×. 
        ×¢×œ×™×š ×œ×”×¡×‘×™×¨ ××ª ××©××¢×•×ª ×”× ×ª×•× ×™× ×‘×¦×•×¨×” ×‘×¨×•×¨×”, ××§×¦×•×¢×™×ª ×•××¢×©×™×ª.
        
        ××“×“ ×”×—×•×¡×Ÿ (RISC) - ××•×“×“ ××ª ×™×›×•×œ×ª ×”×ª×œ××™×“×™× ×œ×”×ª××•×“×“ ×¢× ××ª×’×¨×™× ×•××¦×‘×™ ×œ×—×¥. ×¢×¨×›×™× ×’×‘×•×”×™× ××¢×™×“×™× ×¢×œ ×—×•×¡×Ÿ ×’×‘×•×”.
        
        ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™ (ICI) - ××•×“×“ ××ª ×”×××•× ×” ×©×œ ×”×ª×œ××™×“×™× ×‘×™×›×•×œ×ª× ×œ×©×œ×•×˜ ×‘×—×™×™×”×. ×¢×¨×›×™× ×’×‘×•×”×™× ××¢×™×“×™× ×¢×œ ×ª×—×•×©×ª ×©×œ×™×˜×” ×¢×¦××™×ª.
        
        ×ª×¤×™×¡×•×ª ×–××Ÿ (×’×¨×£ ×¢×›×‘×™×©) - ××¦×™×’ ×—××™×©×” ×××“×™×:
        1. ×¢×‘×¨ ×©×œ×™×œ×™ - ×ª×¤×™×¡×” ×©×œ×™×œ×™×ª ×©×œ ×”×¢×‘×¨, ×˜×¨××•××•×ª ×•×—×•×•×™×•×ª ×§×©×•×ª
        2. ×¢×‘×¨ ×—×™×•×‘×™ - ×ª×¤×™×¡×” ×—×™×•×‘×™×ª ×©×œ ×”×¢×‘×¨, × ×•×¡×˜×œ×’×™×” ×•×–×›×¨×•× ×•×ª ×˜×•×‘×™×
        3. ×”×•×•×” ×“×˜×¨××™× ×™×¡×˜×™ - ×ª×¤×™×¡×” ×¤×˜×œ×™×¡×˜×™×ª ×©×œ ×”×”×•×•×”, ×—×•×¡×¨ ×©×œ×™×˜×”
        4. ×”×•×•×” ×”×“×•× ×™×¡×˜×™ - ×ª×¤×™×¡×ª ×”×•×•×” ×”×“×•× ×™×¡×˜×™×ª, ×—×™×¤×•×© ×”× ××•×ª ××™×™×“×™×•×ª
        5. ×¢×ª×™×“ - ×™×›×•×œ×ª ×ª×›× ×•×Ÿ ×§×“×™××”, ×“×—×™×™×ª ×¡×™×¤×•×§×™×, ×”×¦×‘×ª ××˜×¨×•×ª
        
        ×”×ª×™×™×—×¡ ×‘×”×¡×‘×¨ ×©×œ×š ×œ×”×©×œ×›×•×ª ×”×—×™× ×•×›×™×•×ª ×•×”××¢×©×™×•×ª ×©×œ ×”× ×ª×•× ×™×.
        """
        
        # ×”×©×ª××© ×‘-OpenAI API ×œ×§×‘×œ×ª ×”×¡×‘×¨
        response = openai_client.chat.completions.create(
            model="gpt-4o",  # ××• ××•×“×œ ××—×¨ ×©×ª×¨×¦×” ×œ×”×©×ª××© ×‘×•
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=1000
        )
        
        explanation = response.choices[0].message.content
        
        # ×× ×”×ª×©×•×‘×” ×¨×™×§×” ××• ×§×¦×¨×” ××“×™, ×”×—×–×¨ ×”×¡×‘×¨ ×›×œ×œ×™ ×™×•×ª×¨
        if not explanation or len(explanation) < 20:
            if graph_type == "risc":
                return "×—×•×¡×Ÿ ××ª×™×™×—×¡ ×œ×™×›×•×œ×ª ×©×œ ×ª×œ××™×“×™× ×œ×”×ª××•×“×“ ×¢× ××ª×’×¨×™× ×•××¦×‘×™ ×œ×—×¥. ×›×›×œ ×©×”×¦×™×•×Ÿ ×’×‘×•×” ×™×•×ª×¨, ×›×š ×”×ª×œ××™×“×™× ××¨××™× ×™×›×•×œ×ª ×˜×•×‘×” ×™×•×ª×¨ ×œ×”×ª××•×“×“×•×ª ×¢× ×§×©×™×™×."
            elif graph_type == "ici":
                return "××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™ (ICI) ××ª×™×™×—×¡ ×œ××™×“×” ×©×‘×” ××“× ××××™×Ÿ ×©×”×•× ×©×•×œ×˜ ×‘×—×™×™×•. ×¦×™×•×Ÿ ×’×‘×•×” ××¢×™×“ ×¢×œ ×ª×—×•×©×ª ×©×œ×™×˜×” ×¢×¦××™×ª ×’×‘×•×”×”, ×‘×¢×•×“ ×¦×™×•×Ÿ × ××•×š ××¢×™×“ ×¢×œ ×ª×¤×™×¡×” ×©×’×•×¨××™× ×—×™×¦×•× ×™×™× ×©×•×œ×˜×™× ×‘×—×™×™×•."
            elif graph_type == "spider":
                return "×’×¨×£ ×”×¢×›×‘×™×© ××¦×™×’ ×—××™×©×” ×××“×™× ×©×œ ×ª×¤×™×¡×ª ×–××Ÿ: ×¢×‘×¨ ×©×œ×™×œ×™, ×¢×‘×¨ ×—×™×•×‘×™, ×”×•×•×” ×“×˜×¨××™× ×™×¡×˜×™, ×”×•×•×” ×”×“×•× ×™×¡×˜×™, ×•×¢×ª×™×“. ×”××™×–×•×Ÿ ×‘×™×Ÿ ×××“×™× ××œ×• ××©×¤×™×¢ ×¢×œ ×§×‘×œ×ª ×”×—×œ×˜×•×ª ×•×ª×¤×™×¡×ª ×”×¢×•×œ× ×©×œ ×”×ª×œ××™×“×™×."
        
        return explanation
    
    except Exception as e:
        # ×‘××§×¨×” ×©×œ ×©×’×™××”, ×”×—×–×¨ ×”×¡×‘×¨ ×›×œ×œ×™ ×•×”×•×“×¢×ª ×©×’×™××”
        error_msg = f"×œ× ×”×¦×œ×—× ×• ×œ×™×™×¦×¨ ×”×¡×‘×¨ ××•×˜×•××˜×™. ×©×’×™××”: {str(e)}"
        st.error(error_msg)
        
        # ×”×¡×‘×¨ ×‘×¨×™×¨×ª ××—×“×œ ×œ×¤×™ ×¡×•×’ ×”×’×¨×£
        if graph_type == "risc":
            return "×—×•×¡×Ÿ (RISC): ××•×“×“ ××ª ×”×™×›×•×œ×ª ×©×œ ×”×ª×œ××™×“×™× ×œ×”×ª××•×“×“ ×¢× ××ª×’×¨×™× ×•××¦×‘×™ ×œ×—×¥. ×¦×™×•×Ÿ ×’×‘×•×” ××¢×™×“ ×¢×œ ×™×›×•×œ×ª ×”×ª××•×“×“×•×ª ×˜×•×‘×” ×™×•×ª×¨."
        elif graph_type == "ici":
            return "××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™ (ICI): ××•×“×“ ××ª ×”×××•× ×” ×©×œ ××“× ×©×”×•× ×©×•×œ×˜ ×‘×—×™×™×• ×•×œ× ×’×•×¨××™× ×—×™×¦×•× ×™×™×. ×¦×™×•×Ÿ ×’×‘×•×” ××¢×™×“ ×¢×œ ×ª×—×•×©×ª ××¡×•×’×œ×•×ª ×¢×¦××™×ª ×—×–×§×” ×™×•×ª×¨."
        elif graph_type == "spider":
            return "×’×¨×£ ×ª×¤×™×¡×•×ª ×–××Ÿ: ××¦×™×’ ××ª ×”×—×œ×•×§×” ×©×œ ×ª×¤×™×¡×•×ª ×”×–××Ÿ ×”×©×•× ×•×ª (×¢×‘×¨ ×©×œ×™×œ×™/×—×™×•×‘×™, ×”×•×•×” ×“×˜×¨××™× ×™×¡×˜×™/×”×“×•× ×™×¡×˜×™, ×¢×ª×™×“). ××™×–×•×Ÿ × ×›×•×Ÿ ×‘×™×Ÿ ×”×ª×¤×™×¡×•×ª ×—×©×•×‘ ×œ×”×ª× ×”×’×•×ª ×‘×¨×™××” ×•×”×¦×œ×—×” ×œ×™××•×“×™×ª."
        else:
            return "×œ× ×”×¦×œ×—× ×• ×œ×™×™×¦×¨ ×”×¡×‘×¨ ××•×˜×•××˜×™ ×œ×’×¨×£ ×–×”."

# Main Dashboard
def main():
    st.title("×“××©×‘×•×¨×“ × ×™×ª×•×— × ×ª×•× ×™× ğŸ“Š")
    st.markdown("### × ×™×ª×•×— × ×ª×•× ×™× ×—×™× ×•×›×™×™× ×¢× ×’×¨×¤×™× ×•××™× ×˜×¨××§×¦×™×”")
    
    # ×”×•×¡×¤×ª ×’×™×œ×•×™ ××•×‘×™×™×œ ××©×•×¤×¨ ×¢× JavaScript
    st.markdown(
        """
        <script>
            // ×”×¢×‘×¨×ª ×¨×•×—×‘ ×”×—×œ×•×Ÿ ×œ××¤×œ×™×§×¦×™×”
            window.addEventListener('DOMContentLoaded', function() {
                const width = window.innerWidth;
                // ×”×•×¡×¤×ª ×¡×™××•×Ÿ ×œ×ª×¦×•×’×ª ××•×‘×™×™×œ
                if (width < 768) {
                    document.documentElement.classList.add('mobile-view');
                    document.body.style.setProperty('--screen-width', width + 'px');
                }
            });
            // ×”×ª×××ª ×”×¡×™×™×“×‘×¨ ×œ××•×‘×™×™×œ
            window.addEventListener('resize', function() {
                const width = window.innerWidth;
                if (width < 768) {
                    document.documentElement.classList.add('mobile-view');
                } else {
                    document.documentElement.classList.remove('mobile-view');
                }
                document.body.style.setProperty('--screen-width', width + 'px');
            });
        </script>
        """,
        unsafe_allow_html=True
    )
    
    # ×”×•×¡×¤×ª CSS ×¡×¤×¦×™×¤×™ ×œ××•×‘×™×™×œ ×œ××§×¨×” ×©×‘×• ×™×© ×¦×•×¨×š ×‘×”×ª×××•×ª × ×•×¡×¤×•×ª
    st.markdown(
        """
        <style>
        @media (max-width: 768px) {
            /* ×¢×™×¦×•×‘ ××©×•×¤×¨ ×œ×¡×™×™×“×‘×¨ ×‘××•×‘×™×™×œ */
            [data-testid="stSidebar"] {
                border-radius: 0 !important;
            }
            
            /* ××¨×•×•×—×™× ×˜×•×‘×™× ×™×•×ª×¨ ×¢×‘×•×¨ ×‘×—×™×¨×ª ×‘×™×ª ×¡×¤×¨ ×‘××•×‘×™×™×œ */
            div[data-testid="stSelectbox"] {
                margin-bottom: 1rem !important;
                width: 100% !important;
            }
            
            /* ×©×™×¤×•×¨ ××¨××” ××–×”×¨×•×ª ×‘××•×‘×™×™×œ */
            [data-testid="stAlert"] {
                padding: 8px !important;
            }
        }
        </style>
        """,
        unsafe_allow_html=True
    )
    
    # Load data
    try:
        df = init()
    except Exception as e:
        st.error(f"A××™×¨×¢×” ×©×’×™××” ×‘×˜×¢×™× ×ª ×”× ×ª×•× ×™×: {e}")
        df = pd.DataFrame()  # Empty dataframe as fallback
    
    # ×‘×—×™×¨×ª ×‘×™×ª ×¡×¤×¨ ×‘×—×œ×§ ×”×¢×œ×™×•×Ÿ ×©×œ ×”×¢××•×“
    selected_school = None
    if not df.empty and 'school' in df.columns:
        # ××™×›×œ ×¢× ×¢×™×¦×•×‘ ××•×ª×× ×œ×‘×—×™×¨×ª ×‘×™×ª ×”×¡×¤×¨
        school_selector_container = st.container()
        with school_selector_container:
            st.markdown('<div class="school-selector">', unsafe_allow_html=True)
            st.markdown("<h3 style='text-align: center; margin-bottom: 15px;'>×‘×—×™×¨×ª ×‘×™×ª ×¡×¤×¨</h3>", unsafe_allow_html=True)
            unique_schools = df["school"].unique().tolist()
            selected_school = st.selectbox("×‘×—×¨ ×‘×™×ª ×¡×¤×¨:", unique_schools, key="school_selector_top")
            st.markdown('</div>', unsafe_allow_html=True)
            
        filtered_df = df[df['school'] == selected_school]
        st.session_state.graph_data["selected_school"] = selected_school
    else:
        # Demo data if no real data is available
        filtered_df = df
        st.warning("×œ× × ××¦××• × ×ª×•× ×™× ×œ×¡×™× ×•×Ÿ")
    
    # ×”×•×¡×¤×ª CSS ×¡×¤×¦×™×¤×™ ×œ××•×‘×™×™×œ ×× × ×“×¨×©
    st.markdown(
        """
        <style>
        @media (max-width: 768px) {
            /* ×”×¡×ª×¨×ª ×”×¡×™×™×“×‘×¨ ×‘××•×‘×™×™×œ */
            [data-testid="stSidebar"] {
                display: none;
            }
            
            /* ×”×ª×××ª ×ª×•×›×Ÿ ×”×“×£ ×œ××œ×•× ×¨×•×—×‘ ×”××¡×š */
            .main .block-container {
                padding-left: 1rem;
                padding-right: 1rem;
            }
        }
        </style>
        """,
        unsafe_allow_html=True
    )
    
    # ×‘×“×™×§×” ××•×‘×™×™×œ - ×× ×¨×•×—×‘ ×”××¡×š ×§×˜×Ÿ ×-768px, × ×¦×™×’ ××ª ×”×’×¨×¤×™× ××—×“ ××ª×—×ª ×œ×©× ×™
    # Initialize mobile detection in session state if not already set
    if "is_mobile" not in st.session_state:
        st.session_state.is_mobile = False
        
    # # Option 1: Allow manual toggle for mobile view in sidebar
    # with st.sidebar:
    #     st.session_state.is_mobile = st.checkbox("×ª×¦×•×’×ª ××•×‘×™×™×œ", value=st.session_state.is_mobile)
    
    is_mobile = True #st.session_state.is_mobile
    
    if is_mobile:
        # ×‘××•×‘×™×™×œ - ×›×œ ×’×¨×£ ×‘×§×•×œ×•× ×” × ×¤×¨×“×ª ××—×“ ××ª×—×ª ×œ×©× ×™
        st.markdown("<p style='font-size:0.8rem;color:#666;'>×ª×¦×•×’×ª ××•×‘×™×™×œ ××•×¤×¢×œ×ª</p>", unsafe_allow_html=True)
        row1_col1 = st.container()
        row1_col2 = st.container()
        row1_col3 = st.container()
    else:
        # ×‘×“×¡×§×˜×•×¤ - ×©×œ×•×©×” ×’×¨×¤×™× ×‘×©×•×¨×”
        row1_col1, row1_col2, row1_col3 = st.columns(3)
    
    # Generate sample graphs if no data
    if filtered_df.empty:
        # Sample data for demonstration
        with row1_col1:
            st.markdown("### ××“×“ ×—×•×¡×Ÿ")
            fig1 = generate_sample_gauge()
            st.plotly_chart(fig1, use_container_width=True)
            
            # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×œ×“×•×’××” ×œ×¦'××˜×‘×•×˜
            st.session_state.graph_data["risc"] = {
                "value": 3.7,
                "global_avg": 3.5,
                "research_avg": 3.8
            }
            
            # ×›×¤×ª×•×¨ "××” ×–×” ××•××¨?" ×¢×‘×•×¨ ×’×¨×£ ×”×—×•×¡×Ÿ
            if st.button("××” ×–×” ××•××¨?", key="explain_risc"):
                if not st.session_state.graph_explanations["risc"]["explanation"]:
                    explanation = get_graph_explanation("risc", st.session_state.graph_data["risc"])
                    st.session_state.graph_explanations["risc"]["explanation"] = explanation
                
                st.session_state.graph_explanations["risc"]["show"] = not st.session_state.graph_explanations["risc"]["show"]
            
            # ×”×¦×’×ª ×”×”×¡×‘×¨ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
            if st.session_state.graph_explanations["risc"]["show"]:
                st.markdown(f"""<div class="explanation-box">
                {st.session_state.graph_explanations["risc"]["explanation"]}
                </div>""", unsafe_allow_html=True)
            
        with row1_col2:
            st.markdown("### ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™")
            fig2 = generate_sample_gauge(value=3.4, title="××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™")
            st.plotly_chart(fig2, use_container_width=True)
            
            # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×œ×“×•×’××” ×œ×¦'××˜×‘×•×˜
            st.session_state.graph_data["ici"] = {
                "value": 3.4,
                "global_avg": 3.2,
                "research_avg": 3.6
            }
            
            # ×›×¤×ª×•×¨ "××” ×–×” ××•××¨?" ×¢×‘×•×¨ ×’×¨×£ ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™
            if st.button("××” ×–×” ××•××¨?", key="explain_ici"):
                if not st.session_state.graph_explanations["ici"]["explanation"]:
                    explanation = get_graph_explanation("ici", st.session_state.graph_data["ici"])
                    st.session_state.graph_explanations["ici"]["explanation"] = explanation
                
                st.session_state.graph_explanations["ici"]["show"] = not st.session_state.graph_explanations["ici"]["show"]
            
            # ×”×¦×’×ª ×”×”×¡×‘×¨ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
            if st.session_state.graph_explanations["ici"]["show"]:
                st.markdown(f"""<div class="explanation-box">
                {st.session_state.graph_explanations["ici"]["explanation"]}
                </div>""", unsafe_allow_html=True)
            
        with row1_col3:
            st.markdown("### × ×ª×•× ×™ ×¦×™×¨ ×”×–××Ÿ")
            fig3 = generate_sample_spider()
            st.plotly_chart(fig3, use_container_width=True)
            
            # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×œ×“×•×’××” ×œ×¦'××˜×‘×•×˜
            st.session_state.graph_data["spider"] = {
                "negetive_past": {"current": 3.2, "global": 3.0, "research": 2.8},
                "positive_past": {"current": 4.1, "global": 3.8, "research": 4.0},
                "fatalic_present": {"current": 2.8, "global": 3.0, "research": 2.5},
                "hedonistic_present": {"current": 3.5, "global": 3.2, "research": 3.0},
                "future": {"current": 3.9, "global": 3.7, "research": 4.0},
            }
            
            # ×›×¤×ª×•×¨ "××” ×–×” ××•××¨?" ×¢×‘×•×¨ ×’×¨×£ ×ª×¤×™×¡×•×ª ×–××Ÿ
            if st.button("××” ×–×” ××•××¨?", key="explain_spider"):
                if not st.session_state.graph_explanations["spider"]["explanation"]:
                    explanation = get_graph_explanation("spider", st.session_state.graph_data["spider"])
                    st.session_state.graph_explanations["spider"]["explanation"] = explanation
                
                st.session_state.graph_explanations["spider"]["show"] = not st.session_state.graph_explanations["spider"]["show"]
            
            # ×”×¦×’×ª ×”×”×¡×‘×¨ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
            if st.session_state.graph_explanations["spider"]["show"]:
                st.markdown(f"""<div class="explanation-box">
                {st.session_state.graph_explanations["spider"]["explanation"]}
                </div>""", unsafe_allow_html=True)
    else:
        # Real graphs from data
        try:
            school_info = SchoolInfo(filtered_df)
            
            with row1_col1:
                st.markdown("### ××“×“ ×—×•×¡×Ÿ")
                fig_risc = school_info.get_fig_risc("×—×•×¡×Ÿ")
                st.plotly_chart(fig_risc, use_container_width=True)
                
                # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×××™×ª×™×™× ×œ×¦'××˜×‘×•×˜
                st.session_state.graph_data["risc"] = {
                    "value": school_info.risc,
                    "global_avg": st.session_state.global_average["risc"],
                    "research_avg": st.session_state.research_average["risc"]
                }
                
                # ×›×¤×ª×•×¨ "××” ×–×” ××•××¨?" ×¢×‘×•×¨ ×’×¨×£ ×”×—×•×¡×Ÿ ×”×××™×ª×™
                if st.button("××” ×–×” ××•××¨?", key="explain_risc_real"):
                    if not st.session_state.graph_explanations["risc"]["explanation"]:
                        explanation = get_graph_explanation("risc", st.session_state.graph_data["risc"], school_info)
                        st.session_state.graph_explanations["risc"]["explanation"] = explanation
                    
                    st.session_state.graph_explanations["risc"]["show"] = not st.session_state.graph_explanations["risc"]["show"]
                
                # ×”×¦×’×ª ×”×”×¡×‘×¨ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
                if st.session_state.graph_explanations["risc"]["show"]:
                    st.markdown(f"""<div class="explanation-box">
                    {st.session_state.graph_explanations["risc"]["explanation"]}
                    </div>""", unsafe_allow_html=True)
                
            with row1_col2:
                st.markdown("### ××™×§×•×“ ×©×œ×™×˜×” ×¤× ×™××™")
                fig_ici = school_info.get_fig_ici("××™×§×•×“ ×©×œ×™×˜×”")
                st.plotly_chart(fig_ici, use_container_width=True)
                
                # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×××™×ª×™×™× ×œ×¦'××˜×‘×•×˜
                st.session_state.graph_data["ici"] = {
                    "value": school_info.ici,
                    "global_avg": st.session_state.global_average["ici"],
                    "research_avg": st.session_state.research_average["ici"]
                }
                
                # ×›×¤×ª×•×¨ "××” ×–×” ××•××¨?" ×¢×‘×•×¨ ×’×¨×£ ××™×§×•×“ ×©×œ×™×˜×” ×××™×ª×™
                if st.button("××” ×–×” ××•××¨?", key="explain_ici_real"):
                    if not st.session_state.graph_explanations["ici"]["explanation"]:
                        explanation = get_graph_explanation("ici", st.session_state.graph_data["ici"], school_info)
                        st.session_state.graph_explanations["ici"]["explanation"] = explanation
                    
                    st.session_state.graph_explanations["ici"]["show"] = not st.session_state.graph_explanations["ici"]["show"]
                
                # ×”×¦×’×ª ×”×”×¡×‘×¨ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
                if st.session_state.graph_explanations["ici"]["show"]:
                    st.markdown(f"""<div class="explanation-box">
                    {st.session_state.graph_explanations["ici"]["explanation"]}
                    </div>""", unsafe_allow_html=True)
                
            with row1_col3:
                st.markdown("### ×”×ª×¤×œ×’×•×ª ×œ×¤×™ ×××“×™ ×–××Ÿ")
                fig_spider = school_info.get_fig_spider()
                st.plotly_chart(fig_spider, use_container_width=True)
                
                # ×©××™×¨×ª × ×ª×•× ×™ ×’×¨×£ ×××™×ª×™×™× ×œ×¦'××˜×‘×•×˜
                anigmas_dict = school_info.return_anigmas_result_as_dict()
                st.session_state.graph_data["spider"] = {
                    "future_negetive_past": {
                        "current": anigmas_dict["future_negetive_past"],
                        "global": st.session_state.global_average["future_negetive_past"],
                        "research": st.session_state.research_average["future_negetive_past"]
                    },
                    "future_positive_past": {
                        "current": anigmas_dict["future_positive_past"],
                        "global": st.session_state.global_average["future_positive_past"],
                        "research": st.session_state.research_average["future_positive_past"]
                    },
                    "future_fatalic_present": {
                        "current": anigmas_dict["future_fatalic_present"],
                        "global": st.session_state.global_average["future_fatalic_present"],
                        "research": st.session_state.research_average["future_fatalic_present"]
                    },
                    "future_hedonistic_present": {
                        "current": anigmas_dict["future_hedonistic_present"],
                        "global": st.session_state.global_average["future_hedonistic_present"],
                        "research": st.session_state.research_average["future_hedonistic_present"]
                    },
                    "future_future": {
                        "current": anigmas_dict["future_future"],
                        "global": st.session_state.global_average["future_future"],
                        "research": st.session_state.research_average["future_future"]
                    }
                }
                
                # ×›×¤×ª×•×¨ "××” ×–×” ××•××¨?" ×¢×‘×•×¨ ×’×¨×£ ×ª×¤×™×¡×•×ª ×–××Ÿ ×××™×ª×™
                if st.button("××” ×–×” ××•××¨?", key="explain_spider_real"):
                    if not st.session_state.graph_explanations["spider"]["explanation"]:
                        explanation = get_graph_explanation("spider", st.session_state.graph_data["spider"], school_info)
                        st.session_state.graph_explanations["spider"]["explanation"] = explanation
                    
                    st.session_state.graph_explanations["spider"]["show"] = not st.session_state.graph_explanations["spider"]["show"]
                
                # ×”×¦×’×ª ×”×”×¡×‘×¨ ×× ×”×›×¤×ª×•×¨ × ×œ×—×¥
                if st.session_state.graph_explanations["spider"]["show"]:
                    st.markdown(f"""<div class="explanation-box">
                    {st.session_state.graph_explanations["spider"]["explanation"]}
                    </div>""", unsafe_allow_html=True)
        except Exception as e:
            st.error(f"×©×’×™××” ×‘×¢×ª ×™×¦×™×¨×ª ×”×’×¨×¤×™×: {e}")
    
    # Divider - ×”×•×¢×‘×¨ ×œ××§×•× ×”×–×” ×©×”×•× ××ª×—×ª ×œ×›×œ ×”×’×¨×¤×™× ×•×”×›×¤×ª×•×¨×™×
    st.markdown("---")
    
    # Chatbot section with API key information
    st.markdown("### ×¦'××˜×‘×•×˜ ×œ× ×™×ª×•×— × ×ª×•× ×™× ğŸ¤–")
    
    short=llm_gpt.return_llm_answer(return_highlighted_text(school_info),"","")
    # Show API status
    if api_key:
        st.success(short)
    else:
        st.warning("×œ× × ××¦× API key ×©×œ OpenAI. ×”×¦'××˜×‘×•×˜ ×¢×©×•×™ ×œ× ×œ×¤×¢×•×œ ×›×¨××•×™.")
    
    st.markdown("×©××œ ×©××œ×•×ª ×¢×œ ×”× ×ª×•× ×™× ×”××•×¦×’×™× ×‘×’×¨×¤×™× ××• ×¢×œ ×”×ª×•×¦××•×ª ×”×›×œ×œ×™×•×ª")
    
    # ×”×¦×¢×•×ª ×œ×©××œ×•×ª ×œ××©×ª××©
    suggested_questions = [
        "×”×›×Ÿ ×“×•×— ×× ×”×œ",
    ]
    
    # ×××©×§ ××©×ª××© ×©××¨××” ×”×¦×¢×•×ª ×œ×©××œ×•×ª
    st.markdown("#### ×©××œ×•×ª ×œ×“×•×’××”:")
    cols = st.columns(3)
    for i, question in enumerate(suggested_questions):
        col_idx = i % 3
        if cols[col_idx].button(question, key=f"question_{i}"):
            # Add user message to chat history
            st.session_state.messages.append({"role": "user", "content": question})
            
            # ××¦×™×’×™× ××ª ×©××œ×ª ×”××©×ª××©
            with st.chat_message("user"):
                st.markdown(question)
                
            # ××¦×™×’×™× ××ª ×ª×©×•×‘×ª ×”×¦'××˜×‘×•×˜ ×¢× ×¡×˜×¨×™××™× ×’ ×‘×–××Ÿ ×××ª
            with st.chat_message("assistant"):
                data = filtered_df.to_markdown() if not filtered_df.empty else "××™×Ÿ × ×ª×•× ×™× ×–××™× ×™×"
                system_prompt = return_prompt(school_info) #llm_system_massage_manager.get_first_system_prompt(data)
                
                # ×©×™××•×© ×‘×¡×˜×¨×™××™× ×’ ×¢×‘×•×¨ ×©××œ×•×ª ×“×•×’××”
                response_stream = get_openai_response(
                    question, 
                    system_prompt, 
                    st.session_state.messages[:-1], 
                    st.session_state.graph_data,
                    stream=True
                )
                
                # ××§×•× ×œ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª
                message_placeholder = st.empty()
                full_response = ""
                
                # ×œ×•×œ××” ×¢×œ ××™×¨×•×¢×™ ×”×¡×˜×¨×™× ×-OpenAI ×•×”×¦×’×ª× ×‘×–××Ÿ ×××ª
                for chunk in response_stream:
                    if chunk.choices and hasattr(chunk.choices[0], "delta") and hasattr(chunk.choices[0].delta, "content"):
                        content = chunk.choices[0].delta.content
                        if content:
                            full_response += content
                            # ××¦×™×’ ××ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª ×¢× ×¡××Ÿ ××”×‘×”×‘
                            message_placeholder.markdown(full_response + "â–Œ")
                
                # ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××œ××” ×”×¡×•×¤×™×ª (×œ×œ× ×”×¡××Ÿ)
                message_placeholder.markdown(full_response)
            
            # ×”×•×¡×¤×ª ×”×ª×©×•×‘×” ×œ×”×™×¡×˜×•×¨×™×”
            st.session_state.messages.append({"role": "assistant", "content": full_response})
    
    # ×”×¦×’×ª ×”×™×¡×˜×•×¨×™×™×ª ×¦'××˜ (×¨×§ ××”×™×¡×˜×•×¨×™×” ×§×•×“××ª)
    messages_to_display = st.session_state.messages[:-2] if len(st.session_state.messages) >= 2 else []
    if messages_to_display:
        st.markdown("#### ×”×™×¡×˜×•×¨×™×™×ª ×¦'××˜:")
        chat_container = st.container()
        with chat_container:
            for message in messages_to_display:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
    
    # Input for chat
    if prompt := st.chat_input("×©××œ ×©××œ×” ×¢×œ ×”× ×ª×•× ×™×..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # ××¦×™×’×™× ××ª ×©××œ×ª ×”××©×ª××©
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ××¦×™×’×™× ××ª ×ª×©×•×‘×ª ×”×¦'××˜×‘×•×˜ ×¢× ×¡×˜×¨×™××™× ×’ ×‘×–××Ÿ ×××ª
        with st.chat_message("assistant"):
            # ××§×•× ×œ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª
            message_placeholder = st.empty()
            full_response = ""
            
            # ×§×‘×œ×ª ×ª×©×•×‘×” ××”×¦'××˜×‘×•×˜ ×¢× ×¡×˜×¨×™××™× ×’
            response_stream = get_openai_response(
                prompt, 
                llm_system_massage_manager.get_first_system_prompt(filtered_df.to_markdown() if not filtered_df.empty else "××™×Ÿ × ×ª×•× ×™× ×–××™× ×™×"), 
                st.session_state.messages[:-1], 
                st.session_state.graph_data,
                stream=True
            )
            
            # ×œ×•×œ××” ×¢×œ ××™×¨×•×¢×™ ×”×¡×˜×¨×™× ×•×”×¦×’×ª ×”×ª×©×•×‘×” ×‘×–××Ÿ ×××ª
            for chunk in response_stream:
                if chunk.choices and hasattr(chunk.choices[0], "delta") and hasattr(chunk.choices[0].delta, "content"):
                    content = chunk.choices[0].delta.content
                    if content:
                        full_response += content
                        # ××¦×™×’ ××ª ×”×ª×©×•×‘×” ×”××ª×¢×“×›× ×ª ×¢× ×¡××Ÿ ××”×‘×”×‘
                        message_placeholder.markdown(full_response + "â–Œ")
            
            # ×”×¦×’×ª ×”×ª×©×•×‘×” ×”××œ××” ×”×¡×•×¤×™×ª (×œ×œ× ×”×¡××Ÿ)
            message_placeholder.markdown(full_response)
        
        # ×”×•×¡×¤×ª ×”×ª×©×•×‘×” ×œ×”×™×¡×˜×•×¨×™×”
        st.session_state.messages.append({"role": "assistant", "content": full_response})

# Helper functions for sample graphs if no data is available
def generate_sample_gauge(value=3.7, title="×—×•×¡×Ÿ"):
    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=value,
        title={'text': title, 'font': {'size': 24}},
        gauge={
            'axis': {'range': [1, 5]},
            'bar': {'color': "#437742"},
            'steps': [
                {'range': [1, 2], 'color': "#f4f4f4"},
                {'range': [2, 3], 'color': "#f0f0f0"},
                {'range': [3, 4], 'color': "#e8e8e8"},
                {'range': [4, 5], 'color': "#e0e0e0"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 3.5
            }
        }
    ))
    fig.update_layout(height=300)
    return fig

def generate_sample_bar_chart():
    categories = ["×§×˜×’×•×¨×™×” ×", "×§×˜×’×•×¨×™×” ×‘", "×§×˜×’×•×¨×™×” ×’", "×§×˜×’×•×¨×™×” ×“"]
    values = [4.2, 3.8, 2.5, 3.9]
    
    df = pd.DataFrame({"name": categories, "value": values})
    fig = px.bar(df, x='name', y='value', title="×”×ª×¤×œ×’×•×ª ×¦×™×•× ×™×")
    
    # Add reference lines
    fig.add_hline(y=3.5, line=dict(color="#F37321"), annotation_text="×××•×¦×¢ ××—×§×¨×™")
    fig.add_hline(y=3.2, line=dict(color="#1F3B91"), annotation_text="×××•×¦×¢ ××¨×¦×™")
    
    fig.update_layout(height=300)
    return fig

def generate_sample_spider():
    categories = ["×ª×¤×™×¡×ª ×¢×‘×¨ ××¢×›×‘×ª", "×¢×‘×¨ ×›×ª×©×ª×™×ª ×—×™×•×‘×™×ª", "×“×˜×¨××™× ×¡×˜×™×•×ª",
                 "×¡×™×¤×•×§ ××™×™×“×™", "×¢×ª×™×“"]
    
    fig = go.Figure()
    
    # Current values
    fig.add_trace(go.Scatterpolar(
        r=[3.2, 4.1, 2.8, 3.5, 3.9],
        theta=categories,
        fill='none',
        name='×××•×¦×¢ × ×•×›×—×™',
        line=dict(color='#437742')
    ))
    
    # Global average
    fig.add_trace(go.Scatterpolar(
        r=[3.0, 3.8, 3.0, 3.2, 3.7],
        theta=categories,
        fill='none',
        name='×××•×¦×¢ ××¨×¦×™',
        line=dict(color='#1F3B91')
    ))
    
    # Research average
    fig.add_trace(go.Scatterpolar(
        r=[2.8, 4.0, 2.5, 3.0, 4.0],
        theta=categories,
        fill='none',
        name='×××•×¦×¢ ××—×§×¨×™',
        line=dict(color='#F37321')
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[1, 5]
            )
        ),
        showlegend=True,
        height=300
    )
    
    return fig

# Run the main dashboard
if __name__ == "__main__":
    main()